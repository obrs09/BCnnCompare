{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set()\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "\n",
    "import collections\n",
    "import h5py, sys\n",
    "import gzip\n",
    "import os\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "    \n",
    "\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import time\n",
    "import torch.utils.data\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision\n",
    "import matplotlib\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "image_trans_size = 64\n",
    "batch_size = 5\n",
    "nb_epochs = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "def mkdir(paths):\n",
    "    if not isinstance(paths, (list, tuple)):\n",
    "        paths = [paths]\n",
    "    for path in paths:\n",
    "        if not os.path.isdir(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "suffixes = ['B', 'KB', 'MB', 'GB', 'TB', 'PB']\n",
    "def humansize(nbytes):\n",
    "    i = 0\n",
    "    while nbytes >= 1024 and i < len(suffixes)-1:\n",
    "        nbytes /= 1024.\n",
    "        i += 1\n",
    "    f = ('%.2f' % nbytes)\n",
    "    return '%s%s' % (f, suffixes[i])\n",
    "\n",
    "\n",
    "def get_num_batches(nb_samples, batch_size, roundup=True):\n",
    "    if roundup:\n",
    "        return ((nb_samples + (-nb_samples % batch_size)) / batch_size)  # roundup division\n",
    "    else:\n",
    "        return nb_samples / batch_size\n",
    "\n",
    "def generate_ind_batch(nb_samples, batch_size, random=True, roundup=True):\n",
    "    if random:\n",
    "        ind = np.random.permutation(nb_samples)\n",
    "    else:\n",
    "        ind = range(int(nb_samples))\n",
    "    for i in range(int(get_num_batches(nb_samples, batch_size, roundup))):\n",
    "        yield ind[i * batch_size: (i + 1) * batch_size]\n",
    "\n",
    "def to_variable(var=(), cuda=True, volatile=False):\n",
    "    out = []\n",
    "    for v in var:\n",
    "        if isinstance(v, np.ndarray):\n",
    "            v = torch.from_numpy(v).type(torch.FloatTensor)\n",
    "\n",
    "        if not v.is_cuda and cuda:\n",
    "            v = v.cuda()\n",
    "\n",
    "        if not isinstance(v, Variable):\n",
    "            v = Variable(v, volatile=volatile)\n",
    "\n",
    "        out.append(v)\n",
    "    return out\n",
    "  \n",
    "def cprint(color, text, **kwargs):\n",
    "    if color[0] == '*':\n",
    "        pre_code = '1;'\n",
    "        color = color[1:]\n",
    "    else:\n",
    "        pre_code = ''\n",
    "    code = {\n",
    "        'a': '30',\n",
    "        'r': '31',\n",
    "        'g': '32',\n",
    "        'y': '33',\n",
    "        'b': '34',\n",
    "        'p': '35',\n",
    "        'c': '36',\n",
    "        'w': '37'\n",
    "    }\n",
    "    print(\"\\x1b[%s%sm%s\\x1b[0m\" % (pre_code, code[color], text), **kwargs)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def shuffle_in_unison_scary(a, b):\n",
    "    rng_state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(b)\n",
    "    \n",
    "    \n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datafeed(data.Dataset):\n",
    "\n",
    "    def __init__(self, x_train, y_train, transform=None):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.x_train[index]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, self.y_train[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_train)\n",
    "\n",
    "class DatafeedImage(data.Dataset):\n",
    "    def __init__(self, x_train, y_train, transform=None):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.x_train[index]\n",
    "        img = Image.fromarray(np.uint8(img))\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, self.y_train[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base network wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class BaseNet(object):\n",
    "    def __init__(self):\n",
    "        cprint('c', '\\nNet:')\n",
    "\n",
    "    def get_nb_parameters(self):\n",
    "        return np.sum(p.numel() for p in self.model.parameters())\n",
    "\n",
    "    def set_mode_train(self, train=True):\n",
    "        if train:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "\n",
    "    def update_lr(self, epoch, gamma=0.99):\n",
    "        self.epoch += 1\n",
    "        if self.schedule is not None:\n",
    "            if len(self.schedule) == 0 or epoch in self.schedule:\n",
    "                self.lr *= gamma\n",
    "                print('learning rate: %f  (%d)\\n' % self.lr, epoch)\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr'] = self.lr\n",
    "\n",
    "    def save(self, filename):\n",
    "        cprint('c', 'Writting %s\\n' % filename)\n",
    "        torch.save({\n",
    "            'epoch': self.epoch,\n",
    "            'lr': self.lr,\n",
    "            'model': self.model,\n",
    "            'optimizer': self.optimizer}, filename)\n",
    "\n",
    "    def load(self, filename):\n",
    "        cprint('c', 'Reading %s\\n' % filename)\n",
    "        state_dict = torch.load(filename)\n",
    "        self.epoch = state_dict['epoch']\n",
    "        self.lr = state_dict['lr']\n",
    "        self.model = state_dict['model']\n",
    "        self.optimizer = state_dict['optimizer']\n",
    "        print('  restoring epoch: %d, lr: %f' % (self.epoch, self.lr))\n",
    "        return self.epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_2L(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Linear_2L, self).__init__()\n",
    "        \n",
    "        n_hid = 1200\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, n_hid)\n",
    "        self.fc2 = nn.Linear(n_hid, n_hid)\n",
    "        self.fc3 = nn.Linear(n_hid, output_dim)\n",
    "        \n",
    "        # choose your non linearity\n",
    "        #self.act = nn.Tanh()\n",
    "        #self.act = nn.Sigmoid()\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        #self.act = nn.ELU(inplace=True)\n",
    "        #self.act = nn.SELU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.view(-1, self.input_dim) # view(batch_size, input_dim)\n",
    "        # -----------------\n",
    "        x = self.fc1(x)\n",
    "        # -----------------\n",
    "        x = self.act(x)\n",
    "        # -----------------\n",
    "        x = self.fc2(x)\n",
    "        # -----------------\n",
    "        x = self.act(x)\n",
    "        # -----------------\n",
    "        y = self.fc3(x)\n",
    "\n",
    "        return y\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_groups = list(model.parameters())\n",
    "\n",
    "# for param in param_groups:\n",
    "#     print(param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom SGLD optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.optimizer import Optimizer, required\n",
    "class SGLD(Optimizer):\n",
    "    \"\"\"\n",
    "    SGLD optimiser based on pytorch's SGD. \n",
    "    Note that the weight decay is specified in terms of the gaussian prior sigma\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=required, norm_sigma=0, addnoise=True):\n",
    "        \n",
    "        weight_decay = 1/(norm_sigma**2)\n",
    "        \n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        \n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay, addnoise=addnoise)\n",
    "        \n",
    "        super(SGLD, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        \n",
    "        for group in self.param_groups:\n",
    "\n",
    "            weight_decay = group['weight_decay']\n",
    "            \n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad.data\n",
    "                if weight_decay != 0:\n",
    "                    d_p.add_(weight_decay, p.data)\n",
    "                    \n",
    "                if group['addnoise']:\n",
    "                    \n",
    "                    langevin_noise = p.data.new(p.data.size()).normal_(mean=0, std=1)/np.sqrt(group['lr'])\n",
    "                    p.data.add_(-group['lr'],\n",
    "                                0.5*d_p + langevin_noise)\n",
    "                else:\n",
    "                    p.data.add_(-group['lr'], 0.5*d_p)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pSGLD optimiser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pSGLD(Optimizer):\n",
    "    \"\"\"\n",
    "    RMSprop preconditioned SGLD using pytorch rmsprop implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=required, norm_sigma=0, alpha=0.99, eps=1e-8, centered=False, addnoise=True):\n",
    "        \n",
    "        weight_decay = 1/(norm_sigma**2)\n",
    "        \n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay, alpha=alpha, eps=eps, centered=centered, addnoise=addnoise)\n",
    "        super(pSGLD, self).__init__(params, defaults)\n",
    "        \n",
    "    def __setstate__(self, state):\n",
    "        super(pSGLD, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('centered', False)\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            \n",
    "            weight_decay = group['weight_decay']\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad.data\n",
    "                \n",
    "                state = self.state[p]\n",
    "                \n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['square_avg'] = torch.zeros_like(p.data)\n",
    "                    if group['centered']:\n",
    "                        state['grad_avg'] = torch.zeros_like(p.data)\n",
    "                        \n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "                state['step'] += 1\n",
    "                \n",
    "                if weight_decay != 0:\n",
    "                    d_p.add_(weight_decay, p.data)\n",
    "                \n",
    "                # sqavg x alpha + (1-alph) sqavg *(elemwise) sqavg\n",
    "                square_avg.mul_(alpha).addcmul_(1-alpha, d_p, d_p)\n",
    "                \n",
    "                if group['centered']:\n",
    "                    grad_avg = state['grad_avg']\n",
    "                    grad_avg.mul_(alpha).add_(1-alpha, d_p)\n",
    "                    avg = square_avg.cmul(-1, grad_avg, grad_avg).sqrt().add_(group['eps'])\n",
    "                else:\n",
    "                    avg = square_avg.sqrt().add_(group['eps'])\n",
    "                    \n",
    "#                 print(avg.shape)\n",
    "                if group['addnoise']:\n",
    "                    langevin_noise = p.data.new(p.data.size()).normal_(mean=0, std=1)/np.sqrt(group['lr'])\n",
    "                    p.data.add_(-group['lr'],\n",
    "                                0.5*d_p.div_(avg) + langevin_noise/torch.sqrt(avg))\n",
    "                    \n",
    "                else:\n",
    "                    p.data.addcdiv_(-group['lr'], 0.5*d_p, avg)\n",
    "\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import copy\n",
    "\n",
    "class Net_langevin(BaseNet):\n",
    "    eps = 1e-6\n",
    "\n",
    "    def __init__(self, lr=1e-3, channels_in=1, side_in=224, cuda=True, classes=2, N_train=300, prior_sig=0):\n",
    "        super(Net_langevin, self).__init__()\n",
    "        cprint('y', ' Creating Net!! ')\n",
    "        self.lr = lr\n",
    "        self.schedule = None  # [] #[50,200,400,600]\n",
    "        self.cuda = cuda\n",
    "        self.channels_in = channels_in\n",
    "        self.prior_sig = prior_sig\n",
    "        self.classes = classes\n",
    "        self.N_train = N_train\n",
    "        self.side_in=side_in\n",
    "        self.create_net()\n",
    "        self.create_opt()\n",
    "        self.epoch = 0\n",
    "        \n",
    "        self.weight_set_samples = []\n",
    "\n",
    "        self.test=False\n",
    "\n",
    "    def create_net(self):\n",
    "        torch.manual_seed(42)\n",
    "        if self.cuda:\n",
    "            torch.cuda.manual_seed(42)\n",
    "\n",
    "        self.model = Linear_2L(input_dim = self.channels_in * self.side_in * self.side_in, output_dim=self.classes)\n",
    "        if self.cuda:\n",
    "            self.model.cuda()\n",
    "#             cudnn.benchmark = True\n",
    "\n",
    "        print('    Total params: %.2fM' % (self.get_nb_parameters() / 1000000.0))\n",
    "\n",
    "    def create_opt(self):\n",
    "#         self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, betas=(0.9, 0.999), eps=1e-08,\n",
    "#                                           weight_decay=0)\n",
    "        self.optimizer = SGLD(params=self.model.parameters(), lr=self.lr, norm_sigma=self.prior_sig, addnoise=True)\n",
    "#         self.optimizer = pSGLD(params=self.model.parameters(), lr=self.lr, norm_sigma=self.prior_sig, addnoise=True)\n",
    "\n",
    "    #         self.optimizer = torch.optim.SGD(self.model.parameters(), lr=self.lr, momentum=0.9)\n",
    "#         self.sched = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=1, gamma=10, last_epoch=-1)\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        out = self.model(x)\n",
    "        loss = F.cross_entropy(out, y, reduction='mean') # We use mean because we treat as an estimation of whole dataset\n",
    "        loss = loss * self.N_train \n",
    "            \n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # out: (batch_size, out_channels, out_caps_dims)\n",
    "        pred = out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n",
    "        err = pred.ne(y.data).sum()\n",
    "\n",
    "        return loss.data*x.shape[0]/self.N_train, err\n",
    "\n",
    "    def eval(self, x, y, train=False):\n",
    "        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n",
    "\n",
    "        out = self.model(x)\n",
    "\n",
    "        loss = F.cross_entropy(out, y, reduction='sum')\n",
    "\n",
    "        probs = F.softmax(out, dim=1).data.cpu()\n",
    "\n",
    "        pred = out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n",
    "        err = pred.ne(y.data).sum()\n",
    "\n",
    "        return loss.data, err, probs\n",
    "    \n",
    "    def save_sampled_net(self, max_samples):\n",
    "        \n",
    "        if len(self.weight_set_samples) >= max_samples:\n",
    "            self.weight_set_samples.pop(0)\n",
    "            \n",
    "        self.weight_set_samples.append(copy.deepcopy(self.model.state_dict()))\n",
    "        \n",
    "        cprint('c', ' saving weight samples %d/%d' % (len(self.weight_set_samples), max_samples) )\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    def sample_eval(self, x, y, Nsamples=0, logits=True, train=False):\n",
    "        if Nsamples == 0:\n",
    "            Nsamples = len(self.weight_set_samples)\n",
    "            \n",
    "        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n",
    "        \n",
    "        out = x.data.new(Nsamples, x.shape[0], self.classes)\n",
    "        \n",
    "        # iterate over all saved weight configuration samples\n",
    "        for idx, weight_dict in enumerate(self.weight_set_samples):\n",
    "            if idx == Nsamples:\n",
    "                break\n",
    "            self.model.load_state_dict(weight_dict)\n",
    "            out[idx] = self.model(x)\n",
    "        \n",
    "        \n",
    "        if logits:\n",
    "            mean_out = out.mean(dim=0, keepdim=False)\n",
    "            loss = F.cross_entropy(mean_out, y, reduction='sum')\n",
    "            probs = F.softmax(mean_out, dim=1).data.cpu()\n",
    "            \n",
    "        else:\n",
    "            mean_out =  F.softmax(out, dim=2).mean(dim=0, keepdim=False)\n",
    "            probs = mean_out.data.cpu()\n",
    "            \n",
    "            log_mean_probs_out = torch.log(mean_out)\n",
    "            loss = F.nll_loss(log_mean_probs_out, y, reduction='sum')\n",
    "\n",
    "        pred = mean_out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n",
    "        err = pred.ne(y.data).sum()\n",
    "\n",
    "        return loss.data, err, probs\n",
    "    \n",
    "    def all_sample_eval(self, x, y, Nsamples):\n",
    "        if Nsamples == 0:\n",
    "            Nsamples = len(self.weight_set_samples)\n",
    "            \n",
    "        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n",
    "        \n",
    "        out = x.data.new(Nsamples, x.shape[0], self.classes)\n",
    "        \n",
    "        # iterate over all saved weight configuration samples\n",
    "        for idx, weight_dict in enumerate(self.weight_set_samples):\n",
    "            if idx == Nsamples:\n",
    "                break\n",
    "            self.model.load_state_dict(weight_dict)\n",
    "            out[idx] = self.model(x)\n",
    "        \n",
    "        prob_out =  F.softmax(out, dim=2)\n",
    "        prob_out = prob_out.data\n",
    "\n",
    "        return prob_out\n",
    "    \n",
    "    def get_weight_samples(self, Nsamples=0):\n",
    "        weight_vec = []\n",
    "        \n",
    "        if Nsamples == 0 or Nsamples > len(self.weight_set_samples):\n",
    "            Nsamples = len(self.weight_set_samples)\n",
    "            \n",
    "        for idx, state_dict in enumerate(self.weight_set_samples):\n",
    "            if idx == Nsamples:\n",
    "                break\n",
    "                \n",
    "            for key in state_dict.keys():\n",
    "                if 'weight' in key:\n",
    "                    weight_mtx = state_dict[key].cpu().data\n",
    "                    for weight in weight_mtx.view(-1):\n",
    "                        weight_vec.append(weight)\n",
    "            \n",
    "        return np.array(weight_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "transform_covid19 = transforms.Compose([\n",
    "        #transforms.RandomHorizontalFlip(),\n",
    "        #transforms.RandomVerticalFlip(),\n",
    "        #transforms.RandomRotation(degrees=(0, 180)),\n",
    "        transforms.Resize(image_trans_size),\n",
    "        transforms.CenterCrop(image_trans_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        transforms.Grayscale(num_output_channels=1)\n",
    "])\n",
    "    \n",
    "    \n",
    "trainset = torchvision.datasets.ImageFolder(root=\"../data/COVID/train\", transform=transform_covid19)\n",
    "valset = torchvision.datasets.ImageFolder(root=\"../data/COVID/test\", transform=transform_covid19)\n",
    "num_classes = 2\n",
    "inputs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "print(trainset.targets)\n",
    "print(sum(trainset.targets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0noncovid': 0, '1covid': 1}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.0163302, -2.0163302, -2.0163302, ..., -2.0163302, -2.0163302,\n",
       "        -2.0163302],\n",
       "       [-2.0163302, -2.0163302, -2.0163302, ..., -2.0163302, -2.0163302,\n",
       "        -2.0163302],\n",
       "       [-2.0163302, -2.0163302, -2.0163302, ..., -1.8077447, -1.9641838,\n",
       "        -2.0163302],\n",
       "       ...,\n",
       "       [ 1.7729735,  1.7729735,  1.7729735, ...,  1.7555914,  1.7382094,\n",
       "         1.6860629],\n",
       "       [ 1.7729735,  1.7729735,  1.7729735, ...,  1.7555914,  1.7555914,\n",
       "         1.7208272],\n",
       "       [ 1.7729735,  1.7729735,  1.7729735, ...,  1.7555914,  1.7555914,\n",
       "         1.7208272]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k15 = trainset[0][0].cpu().numpy()\n",
    "k15 = np.concatenate(k15)\n",
    "k15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1deff2cf100>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(k15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_len = len(trainset.targets)\n",
    "test_data_len = len(valset.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\n",
      "Data:\u001b[0m\n",
      "\u001b[36m\n",
      "Network:\u001b[0m\n",
      "\u001b[36m\n",
      "Net:\u001b[0m\n",
      "\u001b[33m Creating Net!! \u001b[0m\n",
      "    Total params: 6.36M\n",
      "\u001b[36m\n",
      "Train:\u001b[0m\n",
      "  init cost variables:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHUAIZ~1\\AppData\\Local\\Temp/ipykernel_29824/2912214083.py:7: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return np.sum(p.numel() for p in self.model.parameters())\n",
      "C:\\Users\\SHUAIZ~1\\AppData\\Local\\Temp/ipykernel_29824/2292766602.py:36: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:1050.)\n",
      "  d_p.add_(weight_decay, p.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 0/256, Jtr_pred = 0.673796, err = 0.403333, \u001b[31m   time: 9.062880 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.685040, err = 0.400000\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "it 1/256, Jtr_pred = 0.800785, err = 0.360000, \u001b[31m   time: 6.528958 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.824738, err = 0.412500\n",
      "\u001b[0m\n",
      "it 2/256, Jtr_pred = 1.375854, err = 0.406667, \u001b[31m   time: 6.373039 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 2.445482, err = 0.500000\n",
      "\u001b[0m\n",
      "it 3/256, Jtr_pred = 1.354033, err = 0.353333, \u001b[31m   time: 6.918093 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 2.441070, err = 0.525000\n",
      "\u001b[0m\n",
      "it 4/256, Jtr_pred = 1.601261, err = 0.323333, \u001b[31m   time: 6.450605 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 3.199146, err = 0.450000\n",
      "\u001b[0m\n",
      "it 5/256, Jtr_pred = 2.247107, err = 0.323333, \u001b[31m   time: 6.631807 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 2.870368, err = 0.475000\n",
      "\u001b[0m\n",
      "it 6/256, Jtr_pred = 2.593292, err = 0.293333, \u001b[31m   time: 6.278807 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 2.091655, err = 0.437500\n",
      "\u001b[0m\n",
      "it 7/256, Jtr_pred = 2.595288, err = 0.323333, \u001b[31m   time: 6.389802 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 2.735255, err = 0.337500\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "it 8/256, Jtr_pred = 1.997540, err = 0.230000, \u001b[31m   time: 6.554022 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 2.955280, err = 0.350000\n",
      "\u001b[0m\n",
      "it 9/256, Jtr_pred = 1.912209, err = 0.250000, \u001b[31m   time: 6.306805 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 9.249814, err = 0.450000\n",
      "\u001b[0m\n",
      "it 10/256, Jtr_pred = 2.542018, err = 0.260000, \u001b[31m   time: 6.257063 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 3.362318, err = 0.350000\n",
      "\u001b[0m\n",
      "it 11/256, Jtr_pred = 3.139231, err = 0.283333, \u001b[31m   time: 6.105124 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 5.045254, err = 0.475000\n",
      "\u001b[0m\n",
      "it 12/256, Jtr_pred = 2.838470, err = 0.230000, \u001b[31m   time: 6.343352 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 5.364570, err = 0.425000\n",
      "\u001b[0m\n",
      "it 13/256, Jtr_pred = 2.138030, err = 0.220000, \u001b[31m   time: 6.306806 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 3.577774, err = 0.412500\n",
      "\u001b[0m\n",
      "it 14/256, Jtr_pred = 2.769166, err = 0.230000, \u001b[31m   time: 6.146609 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 8.957555, err = 0.400000\n",
      "\u001b[0m\n",
      "it 15/256, Jtr_pred = 1.688727, err = 0.200000, \u001b[31m   time: 6.340711 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 2.578246, err = 0.250000\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "it 16/256, Jtr_pred = 3.071053, err = 0.270000, \u001b[31m   time: 6.285806 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 1/90\u001b[0m\n",
      "\u001b[32m    Jdev = 7.045243, err = 0.412500\n",
      "\u001b[0m\n",
      "it 17/256, Jtr_pred = 1.807359, err = 0.206667, \u001b[31m   time: 6.616796 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 5.069333, err = 0.325000\n",
      "\u001b[0m\n",
      "it 18/256, Jtr_pred = 2.325561, err = 0.220000, \u001b[31m   time: 6.213809 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 2/90\u001b[0m\n",
      "\u001b[32m    Jdev = 3.399873, err = 0.300000\n",
      "\u001b[0m\n",
      "it 19/256, Jtr_pred = 2.256093, err = 0.186667, \u001b[31m   time: 6.763085 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 11.017987, err = 0.375000\n",
      "\u001b[0m\n",
      "it 20/256, Jtr_pred = 2.450256, err = 0.193333, \u001b[31m   time: 6.185308 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 3/90\u001b[0m\n",
      "\u001b[32m    Jdev = 3.500909, err = 0.325000\n",
      "\u001b[0m\n",
      "it 21/256, Jtr_pred = 1.473789, err = 0.146667, \u001b[31m   time: 6.321719 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 4.353815, err = 0.350000\n",
      "\u001b[0m\n",
      "it 22/256, Jtr_pred = 2.500079, err = 0.196667, \u001b[31m   time: 6.325019 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 4/90\u001b[0m\n",
      "\u001b[32m    Jdev = 3.852902, err = 0.325000\n",
      "\u001b[0m\n",
      "it 23/256, Jtr_pred = 1.896273, err = 0.183333, \u001b[31m   time: 6.325632 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 7.483913, err = 0.412500\n",
      "\u001b[0m\n",
      "it 24/256, Jtr_pred = 3.136946, err = 0.223333, \u001b[31m   time: 6.142992 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 5/90\u001b[0m\n",
      "\u001b[32m    Jdev = 3.742375, err = 0.287500\n",
      "\u001b[0m\n",
      "it 25/256, Jtr_pred = 2.176997, err = 0.186667, \u001b[31m   time: 6.311291 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 3.939423, err = 0.287500\n",
      "\u001b[0m\n",
      "it 26/256, Jtr_pred = 2.119870, err = 0.160000, \u001b[31m   time: 6.229020 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 6/90\u001b[0m\n",
      "\u001b[32m    Jdev = 5.674618, err = 0.350000\n",
      "\u001b[0m\n",
      "it 27/256, Jtr_pred = 2.554062, err = 0.173333, \u001b[31m   time: 6.151595 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 5.740874, err = 0.312500\n",
      "\u001b[0m\n",
      "it 28/256, Jtr_pred = 2.464382, err = 0.200000, \u001b[31m   time: 6.269123 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 7/90\u001b[0m\n",
      "\u001b[32m    Jdev = 2.835663, err = 0.212500\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "it 29/256, Jtr_pred = 3.192976, err = 0.213333, \u001b[31m   time: 6.329803 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 4.112582, err = 0.325000\n",
      "\u001b[0m\n",
      "it 30/256, Jtr_pred = 2.739467, err = 0.183333, \u001b[31m   time: 6.272804 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 8/90\u001b[0m\n",
      "\u001b[32m    Jdev = 6.812256, err = 0.425000\n",
      "\u001b[0m\n",
      "it 31/256, Jtr_pred = 3.196903, err = 0.206667, \u001b[31m   time: 5.912520 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 14.103052, err = 0.437500\n",
      "\u001b[0m\n",
      "it 32/256, Jtr_pred = 3.383834, err = 0.210000, \u001b[31m   time: 6.008120 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 9/90\u001b[0m\n",
      "\u001b[32m    Jdev = 5.681053, err = 0.350000\n",
      "\u001b[0m\n",
      "it 33/256, Jtr_pred = 2.544258, err = 0.183333, \u001b[31m   time: 6.333483 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 4.794350, err = 0.275000\n",
      "\u001b[0m\n",
      "it 34/256, Jtr_pred = 1.662535, err = 0.133333, \u001b[31m   time: 6.985887 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 10/90\u001b[0m\n",
      "\u001b[32m    Jdev = 5.998235, err = 0.350000\n",
      "\u001b[0m\n",
      "it 35/256, Jtr_pred = 1.942772, err = 0.126667, \u001b[31m   time: 6.436154 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 4.591599, err = 0.300000\n",
      "\u001b[0m\n",
      "it 36/256, Jtr_pred = 1.582005, err = 0.136667, \u001b[31m   time: 6.241811 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 11/90\u001b[0m\n",
      "\u001b[32m    Jdev = 10.333101, err = 0.425000\n",
      "\u001b[0m\n",
      "it 37/256, Jtr_pred = 2.332274, err = 0.203333, \u001b[31m   time: 6.313804 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 10.253354, err = 0.437500\n",
      "\u001b[0m\n",
      "it 38/256, Jtr_pred = 2.594407, err = 0.173333, \u001b[31m   time: 6.481800 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 12/90\u001b[0m\n",
      "\u001b[32m    Jdev = 3.835537, err = 0.300000\n",
      "\u001b[0m\n",
      "it 39/256, Jtr_pred = 1.521013, err = 0.100000, \u001b[31m   time: 6.312436 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 5.943204, err = 0.375000\n",
      "\u001b[0m\n",
      "it 40/256, Jtr_pred = 3.207833, err = 0.226667, \u001b[31m   time: 7.017762 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 13/90\u001b[0m\n",
      "\u001b[32m    Jdev = 8.101370, err = 0.387500\n",
      "\u001b[0m\n",
      "it 41/256, Jtr_pred = 2.112455, err = 0.150000, \u001b[31m   time: 6.259988 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 8.872453, err = 0.375000\n",
      "\u001b[0m\n",
      "it 42/256, Jtr_pred = 1.752925, err = 0.110000, \u001b[31m   time: 6.255322 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 14/90\u001b[0m\n",
      "\u001b[32m    Jdev = 6.201050, err = 0.300000\n",
      "\u001b[0m\n",
      "it 43/256, Jtr_pred = 2.367807, err = 0.160000, \u001b[31m   time: 6.406152 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 15.656644, err = 0.437500\n",
      "\u001b[0m\n",
      "it 44/256, Jtr_pred = 2.581456, err = 0.156667, \u001b[31m   time: 6.635156 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 15/90\u001b[0m\n",
      "\u001b[32m    Jdev = 5.713319, err = 0.325000\n",
      "\u001b[0m\n",
      "it 45/256, Jtr_pred = 1.741830, err = 0.140000, \u001b[31m   time: 6.406837 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 5.464336, err = 0.325000\n",
      "\u001b[0m\n",
      "it 46/256, Jtr_pred = 2.527705, err = 0.150000, \u001b[31m   time: 6.966291 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 16/90\u001b[0m\n",
      "\u001b[32m    Jdev = 7.793610, err = 0.262500\n",
      "\u001b[0m\n",
      "it 47/256, Jtr_pred = 3.001970, err = 0.160000, \u001b[31m   time: 6.576966 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 4.055630, err = 0.212500\n",
      "\u001b[0m\n",
      "it 48/256, Jtr_pred = 2.849722, err = 0.170000, \u001b[31m   time: 6.617395 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 17/90\u001b[0m\n",
      "\u001b[32m    Jdev = 6.674722, err = 0.400000\n",
      "\u001b[0m\n",
      "it 49/256, Jtr_pred = 2.141254, err = 0.156667, \u001b[31m   time: 6.729779 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 6.476318, err = 0.412500\n",
      "\u001b[0m\n",
      "it 50/256, Jtr_pred = 1.684421, err = 0.163333, \u001b[31m   time: 6.887196 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 18/90\u001b[0m\n",
      "\u001b[32m    Jdev = 11.641261, err = 0.400000\n",
      "\u001b[0m\n",
      "it 51/256, Jtr_pred = 1.170057, err = 0.093333, \u001b[31m   time: 6.341379 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 7.002905, err = 0.300000\n",
      "\u001b[0m\n",
      "it 52/256, Jtr_pred = 4.517715, err = 0.186667, \u001b[31m   time: 6.839434 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 19/90\u001b[0m\n",
      "\u001b[32m    Jdev = 12.420882, err = 0.412500\n",
      "\u001b[0m\n",
      "it 53/256, Jtr_pred = 1.787626, err = 0.110000, \u001b[31m   time: 6.167225 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 6.785363, err = 0.362500\n",
      "\u001b[0m\n",
      "it 54/256, Jtr_pred = 2.721205, err = 0.153333, \u001b[31m   time: 6.332807 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 20/90\u001b[0m\n",
      "\u001b[32m    Jdev = 7.612415, err = 0.350000\n",
      "\u001b[0m\n",
      "it 55/256, Jtr_pred = 2.035764, err = 0.150000, \u001b[31m   time: 6.159483 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 6.925946, err = 0.412500\n",
      "\u001b[0m\n",
      "it 56/256, Jtr_pred = 2.482207, err = 0.180000, \u001b[31m   time: 6.480906 seconds\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m saving weight samples 21/90\u001b[0m\n",
      "\u001b[32m    Jdev = 8.227402, err = 0.362500\n",
      "\u001b[0m\n",
      "it 57/256, Jtr_pred = 3.013159, err = 0.173333, \u001b[31m   time: 6.363012 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 3.326008, err = 0.275000\n",
      "\u001b[0m\n",
      "it 58/256, Jtr_pred = 0.936915, err = 0.110000, \u001b[31m   time: 6.305319 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 22/90\u001b[0m\n",
      "\u001b[32m    Jdev = 4.773790, err = 0.212500\n",
      "\u001b[0m\n",
      "it 59/256, Jtr_pred = 2.226723, err = 0.130000, \u001b[31m   time: 6.251807 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 17.966046, err = 0.425000\n",
      "\u001b[0m\n",
      "it 60/256, Jtr_pred = 2.100862, err = 0.156667, \u001b[31m   time: 6.277806 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 23/90\u001b[0m\n",
      "\u001b[32m    Jdev = 7.081643, err = 0.275000\n",
      "\u001b[0m\n",
      "it 61/256, Jtr_pred = 1.381646, err = 0.100000, \u001b[31m   time: 6.677794 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 4.297960, err = 0.262500\n",
      "\u001b[0m\n",
      "it 62/256, Jtr_pred = 4.111375, err = 0.210000, \u001b[31m   time: 6.863139 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 24/90\u001b[0m\n",
      "\u001b[32m    Jdev = 9.671531, err = 0.412500\n",
      "\u001b[0m\n",
      "it 63/256, Jtr_pred = 2.427738, err = 0.150000, \u001b[31m   time: 6.555125 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 5.328130, err = 0.262500\n",
      "\u001b[0m\n",
      "it 64/256, Jtr_pred = 1.746715, err = 0.143333, \u001b[31m   time: 6.381392 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 25/90\u001b[0m\n",
      "\u001b[32m    Jdev = 5.653013, err = 0.312500\n",
      "\u001b[0m\n",
      "it 65/256, Jtr_pred = 1.502142, err = 0.130000, \u001b[31m   time: 6.452802 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 7.566766, err = 0.325000\n",
      "\u001b[0m\n",
      "it 66/256, Jtr_pred = 3.056131, err = 0.170000, \u001b[31m   time: 6.604002 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 26/90\u001b[0m\n",
      "\u001b[32m    Jdev = 5.300515, err = 0.287500\n",
      "\u001b[0m\n",
      "it 67/256, Jtr_pred = 1.761538, err = 0.130000, \u001b[31m   time: 6.259805 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 3.504791, err = 0.237500\n",
      "\u001b[0m\n",
      "it 68/256, Jtr_pred = 2.523103, err = 0.150000, \u001b[31m   time: 6.574660 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 27/90\u001b[0m\n",
      "\u001b[32m    Jdev = 5.485216, err = 0.262500\n",
      "\u001b[0m\n",
      "it 69/256, Jtr_pred = 2.534850, err = 0.150000, \u001b[31m   time: 6.446361 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 6.934933, err = 0.375000\n",
      "\u001b[0m\n",
      "it 70/256, Jtr_pred = 1.778966, err = 0.143333, \u001b[31m   time: 6.546796 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 28/90\u001b[0m\n",
      "\u001b[32m    Jdev = 4.816329, err = 0.337500\n",
      "\u001b[0m\n",
      "it 71/256, Jtr_pred = 2.110923, err = 0.123333, \u001b[31m   time: 6.402236 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 2.728166, err = 0.225000\n",
      "\u001b[0m\n",
      "it 72/256, Jtr_pred = 1.731744, err = 0.150000, \u001b[31m   time: 6.293349 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 29/90\u001b[0m\n",
      "\u001b[32m    Jdev = 6.176230, err = 0.350000\n",
      "\u001b[0m\n",
      "it 73/256, Jtr_pred = 2.769528, err = 0.176667, \u001b[31m   time: 6.667793 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 4.861344, err = 0.275000\n",
      "\u001b[0m\n",
      "it 74/256, Jtr_pred = 1.651913, err = 0.130000, \u001b[31m   time: 5.973814 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 30/90\u001b[0m\n",
      "\u001b[32m    Jdev = 4.134995, err = 0.200000\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "it 75/256, Jtr_pred = 2.822062, err = 0.170000, \u001b[31m   time: 6.615720 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 4.655669, err = 0.300000\n",
      "\u001b[0m\n",
      "it 76/256, Jtr_pred = 1.857973, err = 0.150000, \u001b[31m   time: 6.271148 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 31/90\u001b[0m\n",
      "\u001b[32m    Jdev = 6.297304, err = 0.275000\n",
      "\u001b[0m\n",
      "it 77/256, Jtr_pred = 3.302898, err = 0.186667, \u001b[31m   time: 6.286388 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 4.611582, err = 0.275000\n",
      "\u001b[0m\n",
      "it 78/256, Jtr_pred = 1.748541, err = 0.113333, \u001b[31m   time: 6.394864 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 32/90\u001b[0m\n",
      "\u001b[32m    Jdev = 7.558147, err = 0.387500\n",
      "\u001b[0m\n",
      "it 79/256, Jtr_pred = 1.065779, err = 0.080000, \u001b[31m   time: 6.067925 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 5.205764, err = 0.287500\n",
      "\u001b[0m\n",
      "it 80/256, Jtr_pred = 1.798342, err = 0.116667, \u001b[31m   time: 6.238806 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 33/90\u001b[0m\n",
      "\u001b[32m    Jdev = 4.938678, err = 0.325000\n",
      "\u001b[0m\n",
      "it 81/256, Jtr_pred = 2.235572, err = 0.153333, \u001b[31m   time: 6.415802 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 6.480194, err = 0.337500\n",
      "\u001b[0m\n",
      "it 82/256, Jtr_pred = 2.699185, err = 0.170000, \u001b[31m   time: 6.237806 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 34/90\u001b[0m\n",
      "\u001b[32m    Jdev = 11.337569, err = 0.412500\n",
      "\u001b[0m\n",
      "it 83/256, Jtr_pred = 2.152697, err = 0.130000, \u001b[31m   time: 6.374801 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 8.869454, err = 0.400000\n",
      "\u001b[0m\n",
      "it 84/256, Jtr_pred = 2.247064, err = 0.153333, \u001b[31m   time: 6.812790 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 35/90\u001b[0m\n",
      "\u001b[32m    Jdev = 6.782179, err = 0.325000\n",
      "\u001b[0m\n",
      "it 85/256, Jtr_pred = 3.989795, err = 0.166667, \u001b[31m   time: 7.549149 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 9.447346, err = 0.437500\n",
      "\u001b[0m\n",
      "it 86/256, Jtr_pred = 1.977592, err = 0.126667, \u001b[31m   time: 7.331907 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 36/90\u001b[0m\n",
      "\u001b[32m    Jdev = 5.682154, err = 0.350000\n",
      "\u001b[0m\n",
      "it 87/256, Jtr_pred = 1.325724, err = 0.116667, \u001b[31m   time: 6.804052 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 7.244904, err = 0.350000\n",
      "\u001b[0m\n",
      "it 88/256, Jtr_pred = 2.015489, err = 0.146667, \u001b[31m   time: 6.681138 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 37/90\u001b[0m\n",
      "\u001b[32m    Jdev = 12.452088, err = 0.425000\n",
      "\u001b[0m\n",
      "it 89/256, Jtr_pred = 2.413976, err = 0.150000, \u001b[31m   time: 6.732065 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 8.097358, err = 0.400000\n",
      "\u001b[0m\n",
      "it 90/256, Jtr_pred = 2.416026, err = 0.130000, \u001b[31m   time: 7.458432 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 38/90\u001b[0m\n",
      "\u001b[32m    Jdev = 7.386406, err = 0.362500\n",
      "\u001b[0m\n",
      "it 91/256, Jtr_pred = 2.764989, err = 0.146667, \u001b[31m   time: 6.989866 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 5.557357, err = 0.300000\n",
      "\u001b[0m\n",
      "it 92/256, Jtr_pred = 1.529328, err = 0.133333, \u001b[31m   time: 7.062344 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 39/90\u001b[0m\n",
      "\u001b[32m    Jdev = 9.664366, err = 0.400000\n",
      "\u001b[0m\n",
      "it 93/256, Jtr_pred = 2.721380, err = 0.166667, \u001b[31m   time: 6.671793 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 12.156751, err = 0.437500\n",
      "\u001b[0m\n",
      "it 94/256, Jtr_pred = 2.708240, err = 0.170000, \u001b[31m   time: 8.180887 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 40/90\u001b[0m\n",
      "\u001b[32m    Jdev = 7.010759, err = 0.375000\n",
      "\u001b[0m\n",
      "it 95/256, Jtr_pred = 2.874558, err = 0.163333, \u001b[31m   time: 7.049196 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 7.163947, err = 0.375000\n",
      "\u001b[0m\n",
      "it 96/256, Jtr_pred = 2.821375, err = 0.196667, \u001b[31m   time: 6.422597 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 41/90\u001b[0m\n",
      "\u001b[32m    Jdev = 6.685793, err = 0.337500\n",
      "\u001b[0m\n",
      "it 97/256, Jtr_pred = 1.856574, err = 0.113333, \u001b[31m   time: 7.030169 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 8.095116, err = 0.325000\n",
      "\u001b[0m\n",
      "it 98/256, Jtr_pred = 1.623732, err = 0.130000, \u001b[31m   time: 6.625562 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 42/90\u001b[0m\n",
      "\u001b[32m    Jdev = 17.745630, err = 0.437500\n",
      "\u001b[0m\n",
      "it 99/256, Jtr_pred = 2.006829, err = 0.130000, \u001b[31m   time: 7.123782 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 5.094875, err = 0.237500\n",
      "\u001b[0m\n",
      "it 100/256, Jtr_pred = 2.390161, err = 0.180000, \u001b[31m   time: 6.502801 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 43/90\u001b[0m\n",
      "\u001b[32m    Jdev = 4.848578, err = 0.312500\n",
      "\u001b[0m\n",
      "it 101/256, Jtr_pred = 3.256290, err = 0.196667, \u001b[31m   time: 6.450262 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 14.765536, err = 0.412500\n",
      "\u001b[0m\n",
      "it 102/256, Jtr_pred = 1.325600, err = 0.116667, \u001b[31m   time: 6.514030 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 44/90\u001b[0m\n",
      "\u001b[32m    Jdev = 3.717621, err = 0.237500\n",
      "\u001b[0m\n",
      "it 103/256, Jtr_pred = 2.594533, err = 0.143333, \u001b[31m   time: 6.390282 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 5.239293, err = 0.337500\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "models_dir = 'models_SGLD_COVID150'\n",
    "results_dir = 'results_SGLD_COVID150'\n",
    "\n",
    "mkdir(models_dir)\n",
    "mkdir(results_dir)\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# train config\n",
    "NTrainPoints = train_data_len\n",
    "#batch_size = 20\n",
    "#nb_epochs = 100 # We can do less iterations as this method has faster convergence\n",
    "log_interval = 1\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# dataset\n",
    "cprint('c', '\\nData:')\n",
    "\n",
    "\n",
    "# load data\n",
    "\n",
    "# data augmentation\n",
    "# transform_train = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "# ])\n",
    "\n",
    "# transform_test = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "# ])\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# trainset = datasets.MNIST(root='../data', train=True, download=True, transform=transform_train)\n",
    "# valset = datasets.MNIST(root='../data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "if use_cuda:\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=3)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=3)\n",
    "\n",
    "else:\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=False,\n",
    "                                              num_workers=3)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n",
    "                                            num_workers=3)\n",
    "\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# net dims\n",
    "cprint('c', '\\nNetwork:')\n",
    "\n",
    "lr = 1e-5\n",
    "prior_sig = 0.1\n",
    "########################################################################################\n",
    "net = Net_langevin(lr=lr, channels_in=1, side_in=image_trans_size, cuda=use_cuda, classes=2, N_train=NTrainPoints, prior_sig=prior_sig)\n",
    "\n",
    "epoch = 0\n",
    "\n",
    "## weight saving parameters #######\n",
    "start_save = 15\n",
    "save_every = 2 # We want less correlated samples -> despite having per minibatch noise we see correlations\n",
    "N_saves = 90\n",
    "###################################\n",
    "\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# train\n",
    "cprint('c', '\\nTrain:')\n",
    "\n",
    "print('  init cost variables:')\n",
    "pred_cost_train = np.zeros(nb_epochs)\n",
    "err_train = np.zeros(nb_epochs)\n",
    "\n",
    "cost_dev = np.zeros(nb_epochs)\n",
    "err_dev = np.zeros(nb_epochs)\n",
    "# best_cost = np.inf\n",
    "best_err = np.inf\n",
    "\n",
    "\n",
    "nb_its_dev = 1\n",
    "\n",
    "tic0 = time.time()\n",
    "for i in range(epoch, nb_epochs):\n",
    "    \n",
    "#     if i in [1]:\n",
    "#         print('updating lr')\n",
    "#         net.sched.step()\n",
    "    \n",
    "    net.set_mode_train(True)\n",
    "\n",
    "    tic = time.time()\n",
    "    nb_samples = 0\n",
    "\n",
    "    for x, y in trainloader:\n",
    "\n",
    "        cost_pred, err = net.fit(x, y)\n",
    "\n",
    "        err_train[i] += err\n",
    "        pred_cost_train[i] += cost_pred\n",
    "        nb_samples += len(x)\n",
    "\n",
    "    pred_cost_train[i] /= nb_samples\n",
    "    err_train[i] /= nb_samples\n",
    "\n",
    "    toc = time.time()\n",
    "    net.epoch = i\n",
    "    # ---- print\n",
    "    print(\"it %d/%d, Jtr_pred = %f, err = %f, \" % (i, nb_epochs, pred_cost_train[i], err_train[i]), end=\"\")\n",
    "    cprint('r', '   time: %f seconds\\n' % (toc - tic))\n",
    "    \n",
    "    # ---- save weights\n",
    "    if i >= start_save and i % save_every == 0:\n",
    "        net.save_sampled_net(max_samples=N_saves)\n",
    "\n",
    "    # ---- dev\n",
    "    if i % nb_its_dev == 0:\n",
    "        net.set_mode_train(False)\n",
    "        nb_samples = 0\n",
    "        for j, (x, y) in enumerate(valloader):\n",
    "\n",
    "            cost, err, probs = net.eval(x, y)\n",
    "\n",
    "            cost_dev[i] += cost\n",
    "            err_dev[i] += err\n",
    "            nb_samples += len(x)\n",
    "\n",
    "        cost_dev[i] /= nb_samples\n",
    "        err_dev[i] /= nb_samples\n",
    "\n",
    "        cprint('g', '    Jdev = %f, err = %f\\n' % (cost_dev[i], err_dev[i]))\n",
    "\n",
    "        if err_dev[i] < best_err:\n",
    "            best_err = err_dev[i]\n",
    "            cprint('b', 'best test error')\n",
    "#             net.save(models_dir+'/theta_best.dat')\n",
    "\n",
    "toc0 = time.time()\n",
    "runtime_per_it = (toc0 - tic0) / float(nb_epochs)\n",
    "cprint('r', '   average time: %f seconds\\n' % runtime_per_it)\n",
    "\n",
    "\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# results\n",
    "cprint('c', '\\nRESULTS:')\n",
    "nb_parameters = net.get_nb_parameters()\n",
    "best_cost_dev = np.min(cost_dev)\n",
    "best_cost_train = np.min(pred_cost_train)\n",
    "err_dev_min = err_dev[::nb_its_dev].min()\n",
    "\n",
    "print('  cost_dev: %f (cost_train %f)' % (best_cost_dev, best_cost_train))\n",
    "print('  err_dev: %f' % (err_dev_min))\n",
    "print('  nb_parameters: %d (%s)' % (nb_parameters, humansize(nb_parameters)))\n",
    "print('  time_per_it: %fs\\n' % (runtime_per_it))\n",
    "\n",
    "\n",
    "\n",
    "## Save results for plots\n",
    "# np.save('results/test_predictions.npy', test_predictions)\n",
    "np.save(results_dir + '/cost_train.npy', pred_cost_train)\n",
    "np.save(results_dir + '/cost_dev.npy', cost_dev)\n",
    "np.save(results_dir + '/err_train.npy', err_train)\n",
    "np.save(results_dir + '/err_dev.npy', err_dev)\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# fig cost vs its\n",
    "\n",
    "textsize = 15\n",
    "marker=5\n",
    "\n",
    "\n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=100)\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(range(0, nb_epochs, nb_its_dev), cost_dev[::nb_its_dev], 'b-')\n",
    "ax1.plot(pred_cost_train, 'r--')\n",
    "ax1.set_ylabel('Cross Entropy')\n",
    "plt.xlabel('epoch')\n",
    "plt.grid(b=True, which='major', color='k', linestyle='-')\n",
    "plt.grid(b=True, which='minor', color='k', linestyle='--')\n",
    "lgd = plt.legend(['test error', 'train error'], markerscale=marker, prop={'size': textsize, 'weight': 'normal'})\n",
    "ax = plt.gca()\n",
    "plt.title('classification costs')\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "    ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(textsize)\n",
    "    item.set_weight('normal')\n",
    "#plt.savefig(results_dir + '/cost.png', bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.set_ylabel('% error')\n",
    "ax2.plot(range(0, nb_epochs, nb_its_dev), 100 * err_dev[::nb_its_dev], 'b-')\n",
    "ax2.plot(100 * err_train, 'r--')\n",
    "plt.xlabel('epoch')\n",
    "plt.grid(b=True, which='major', color='k', linestyle='-')\n",
    "plt.grid(b=True, which='minor', color='k', linestyle='--')\n",
    "ax2.get_yaxis().set_minor_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "ax2.get_yaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "lgd = plt.legend(['test error', 'train error'], markerscale=marker, prop={'size': textsize, 'weight': 'normal'})\n",
    "ax = plt.gca()\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "    ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(textsize)\n",
    "    item.set_weight('normal')\n",
    "#plt.savefig(results_dir + '/err.png',  bbox_extra_artists=(lgd,), box_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "textsize = 15\n",
    "marker=5\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(range(0, nb_epochs, nb_its_dev), cost_dev[::nb_its_dev], 'b-')\n",
    "ax1.plot(pred_cost_train, 'r--')\n",
    "ax1.set_ylabel('Cross Entropy')\n",
    "plt.xlabel('epoch')\n",
    "plt.grid(b=True, which='major', color='k', linestyle='-')\n",
    "plt.grid(b=True, which='minor', color='k', linestyle='--')\n",
    "lgd = plt.legend(['test error', 'train error'], markerscale=marker, prop={'size': textsize, 'weight': 'normal'})\n",
    "ax = plt.gca()\n",
    "plt.title('classification costs')\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "    ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(textsize)\n",
    "    item.set_weight('normal')\n",
    "plt.savefig(results_dir + '/cost.png', bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.set_ylabel('% error')\n",
    "ax2.plot(range(0, nb_epochs, nb_its_dev), 100 * err_dev[::nb_its_dev], 'b-')\n",
    "ax2.plot(100 * err_train, 'r--')\n",
    "plt.xlabel('epoch')\n",
    "plt.grid(b=True, which='major', color='k', linestyle='-')\n",
    "plt.grid(b=True, which='minor', color='k', linestyle='--')\n",
    "ax2.get_yaxis().set_minor_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "ax2.get_yaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "lgd = plt.legend(['test error', 'train error'], markerscale=marker, prop={'size': textsize, 'weight': 'normal'})\n",
    "ax = plt.gca()\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "    ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(textsize)\n",
    "    item.set_weight('normal')\n",
    "plt.savefig(results_dir + '/err.png',  bbox_extra_artists=(lgd,), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "save_object(net.weight_set_samples, models_dir+'/state_dicts.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import time\n",
    "import torch.utils.data\n",
    "from torchvision import transforms, datasets\n",
    "import matplotlib\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "models_dir = 'models_SGLD_COVID150'\n",
    "results_dir = 'results_SGLD_COVID150'\n",
    "\n",
    "mkdir(models_dir)\n",
    "mkdir(results_dir)\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# train config\n",
    "#NTrainPointsMNIST = 300\n",
    "#batch_size = 128\n",
    "#nb_epochs = 200 # We can do less iterations as this method has faster convergence\n",
    "log_interval = 1\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# dataset\n",
    "cprint('c', '\\nData:')\n",
    "\n",
    "\n",
    "# load data\n",
    "\n",
    "# data augmentation\n",
    "# transform_train = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "# ])\n",
    "\n",
    "# transform_test = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "# ])\n",
    "\n",
    "# use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# trainset = datasets.MNIST(root='../data', train=True, download=True, transform=transform_train)\n",
    "# valset = datasets.MNIST(root='../data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "transform_covid19 = transforms.Compose([\n",
    "        #transforms.RandomHorizontalFlip(),\n",
    "        #transforms.RandomVerticalFlip(),\n",
    "        #transforms.RandomRotation(degrees=(0, 180)),\n",
    "        transforms.Resize(image_trans_size),\n",
    "        transforms.CenterCrop(image_trans_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        transforms.Grayscale(num_output_channels=1)\n",
    "])\n",
    "    \n",
    "    \n",
    "trainset = torchvision.datasets.ImageFolder(root=\"../data/COVID/train\", transform=transform_covid19)\n",
    "valset = torchvision.datasets.ImageFolder(root=\"../data/COVID/test\", transform=transform_covid19)\n",
    "num_classes = 2\n",
    "inputs = 3\n",
    "\n",
    "\n",
    "if use_cuda:\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=3)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=3)\n",
    "\n",
    "else:\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=False,\n",
    "                                              num_workers=3)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n",
    "                                            num_workers=3)\n",
    "\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# net dims\n",
    "cprint('c', '\\nNetwork:')\n",
    "\n",
    "lr = 1e-5\n",
    "prior_sig = 0.1\n",
    "########################################################################################\n",
    "net = Net_langevin(lr=lr, channels_in=1, side_in=image_trans_size, cuda=use_cuda, classes=2, N_train=NTrainPoints, prior_sig=prior_sig)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open(models_dir+'/state_dicts.pkl', 'rb') as input:\n",
    "    net.weight_set_samples = pickle.load(input)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference with sampling on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size = 200\n",
    "\n",
    "if use_cuda:\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=4)\n",
    "\n",
    "else:\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n",
    "                                            num_workers=4)\n",
    "test_cost = 0  # Note that these are per sample\n",
    "test_err = 0\n",
    "nb_samples = 0\n",
    "test_predictions = np.zeros((test_data_len, 2))\n",
    "\n",
    "Nsamples = 90\n",
    "\n",
    "net.set_mode_train(False)\n",
    "\n",
    "for j, (x, y) in enumerate(valloader):\n",
    "    cost, err, probs = net.sample_eval(x, y, Nsamples, logits=False) # , logits=True\n",
    "\n",
    "    test_cost += cost\n",
    "    test_err += err.cpu().numpy()\n",
    "    test_predictions[nb_samples:nb_samples+len(x), :] = probs.numpy()\n",
    "    nb_samples += len(x)\n",
    "\n",
    "# test_cost /= nb_samples\n",
    "test_err /= nb_samples\n",
    "cprint('b', '    Loglike = %5.6f, err = %1.6f\\n' % (-test_cost, test_err))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage as ndim\n",
    "import matplotlib.colors as mcolors\n",
    "x_dev = []\n",
    "y_dev = []\n",
    "for x, y in valloader:\n",
    "    x_dev.append(x.cpu().numpy())\n",
    "    y_dev.append(y.cpu().numpy())\n",
    "\n",
    "x_dev = np.concatenate(x_dev)\n",
    "y_dev = np.concatenate(y_dev)\n",
    "print(x_dev.shape)\n",
    "print(y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "im_ind = np.random.randint(0, y_dev.shape[0])\n",
    "#im_ind = 90\n",
    "print(\"image number:\", im_ind)\n",
    "\n",
    "\n",
    "x, y = x_dev[im_ind], y_dev[im_ind]\n",
    "x_rot = np.expand_dims(ndim.interpolation.rotate(x[0, :, :], 0, reshape=False, cval=-0.42421296), 0)\n",
    "\n",
    "print(\"real number:\",y)\n",
    "\n",
    "plt.imshow( ndim.interpolation.rotate(x_dev[im_ind,0,:,:], 0, reshape=False))\n",
    "plt.show()\n",
    "\n",
    "ims=[]\n",
    "\n",
    "\n",
    "ims.append(x_rot[:,:,:])\n",
    "\n",
    "\n",
    "ims = np.concatenate(ims)\n",
    "\n",
    "net.set_mode_train(False)\n",
    "\n",
    "y = np.ones(ims.shape[0])*y\n",
    "ims = np.expand_dims(ims, axis=1)\n",
    "\n",
    "cost, err, probs = net.sample_eval(torch.from_numpy(ims), torch.from_numpy(y), Nsamples=Nsamples, logits=False) # , logits=True\n",
    "\n",
    "predictions = probs.numpy()\n",
    "\n",
    "print(\"predictions\", predictions)\n",
    "\n",
    "print(\"error\", err.cpu().numpy())\n",
    "\n",
    "\n",
    "# predictions.max(axis=1)[0]\n",
    "# selections = (predictions[:,i] == predictions.max(axis=1))\n",
    "print(\"predict\", predictions.argmax())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "im_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valset[im_ind][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "prob = []\n",
    "for i in range(0,test_data_len):\n",
    "    x, y = x_dev[i], y_dev[i]\n",
    "    x_rot = np.expand_dims(ndim.interpolation.rotate(x[0, :, :], 0, reshape=False, cval=-0.42421296), 0)\n",
    "    #print(\"real number:\",y)\n",
    "    y_true.append(y)\n",
    "    #plt.imshow( ndim.interpolation.rotate(x_dev[im_ind,0,:,:], 0, reshape=False))\n",
    "    #plt.show()\n",
    "    ims=[]\n",
    "    ims.append(x_rot[:,:,:])\n",
    "    ims = np.concatenate(ims)\n",
    "    net.set_mode_train(False)\n",
    "    y = np.ones(ims.shape[0])*y\n",
    "    ims = np.expand_dims(ims, axis=1)\n",
    "    cost, err, probs = net.sample_eval(torch.from_numpy(ims), torch.from_numpy(y), Nsamples=Nsamples, logits=False) # , logits=True\n",
    "    predictions = probs.numpy()\n",
    "    prob.append(predictions)\n",
    "#     print(\"predictions\", predictions)\n",
    "#     print(\"error\", err.cpu().numpy())\n",
    "    y_pred.append(predictions.argmax())\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "\n",
    "f, ax2 = plt.subplots(figsize = (10, 8), nrows=1)\n",
    "\n",
    "C2= confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "print(C2)\n",
    "print(C2.ravel())\n",
    "sns.heatmap(C2,annot=True)\n",
    "\n",
    "ax2.set_title('confusion_matrix')\n",
    "ax2.set_xlabel('Pred')\n",
    "ax2.set_ylabel('True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Accuracy: \"+str(round((tp+tn)/(tp+fp+fn+tn), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_true, y_pred, pos_label=2)\n",
    "print(\"fpr:{},tpr:{},thresholds:{}\".format(fpr,tpr,thresholds))\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "print(roc_auc)\n",
    "\n",
    "plt.plot(fpr, tpr, lw=1, label=\"TB vs nonTB, area=%0.2f)\" % (roc_auc))\n",
    "\n",
    "plt.xlim([0.00, 1.0])\n",
    "plt.ylim([0.00, 1.0])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(r\"./ROC.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thres = np.arange(0,1,0.01)\n",
    "thres.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = np.array(prob)\n",
    "prob = prob.reshape(80,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnl=[]\n",
    "fpl=[]\n",
    "fnl=[]\n",
    "tpl=[]\n",
    "acc=[]\n",
    "y_true = np.append(np.zeros(40),np.ones(40))\n",
    "for j in range(0, thres.shape[0]):\n",
    "    y_pred=[]\n",
    "    for i in range(0,prob.shape[0]):\n",
    "        if prob[i][1] >= thres[j]:\n",
    "            y_pred.append(1)\n",
    "        else:\n",
    "            y_pred.append(0)\n",
    "    y_pred = np.array(y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "    tnl.append(tn)\n",
    "    fpl.append(fp)\n",
    "    fnl.append(fn)\n",
    "    tpl.append(tp)\n",
    "    acc.append(round((tp+tn)/(tp+fp+fn+tn), 3))\n",
    "    \n",
    "    \n",
    "tnl = np.array(tnl)\n",
    "fpl = np.array(fpl)\n",
    "fnl = np.array(fnl)\n",
    "tpl = np.array(tpl)\n",
    "acc = np.array(acc)\n",
    "\n",
    "plt.plot(thres, tnl, '.')\n",
    "plt.plot(thres, fpl, '-')\n",
    "plt.plot(thres, fnl, 'o')\n",
    "plt.plot(thres, tpl, '-.')\n",
    "plt.plot(thres, acc*100, '*')\n",
    "plt.legend(['True negetive', 'False positive','False negetive','True positive', 'acc'])\n",
    "plt.xlabel(\"threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = np.array([acc, thres,tnl,fpl,fnl,tpl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=np.where(acc==np.max(acc))\n",
    "print('max acc is', acc[c])\n",
    "print('max of acc is at', c)\n",
    "print('corr threshold is',thres[c])\n",
    "print('True negetive is', tnl[c])\n",
    "print('False positive', fpl[c])\n",
    "print('False negetive is', fnl[c])\n",
    "print('True positive is', tpl[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = pd.DataFrame(mat,  index = [' acc', 'threshold', 'TN', 'FP','FN','TP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
