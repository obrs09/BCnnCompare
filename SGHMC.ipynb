{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44bfa61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import time\n",
    "import torch.utils.data\n",
    "from torchvision import transforms, datasets\n",
    "import argparse\n",
    "import matplotlib\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "\n",
    "from __future__ import print_function, division\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "    \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage as ndim\n",
    "import matplotlib.colors as mcolors\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch.utils.data\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision\n",
    "import argparse\n",
    "import matplotlib\n",
    "from src.Stochastic_Gradient_Langevin_Dynamics.model import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "import collections\n",
    "import h5py, sys\n",
    "import gzip\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "\n",
    "\n",
    "import time\n",
    "import torch.utils.data\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision\n",
    "import matplotlib\n",
    "\n",
    "import time\n",
    "import torch.utils.data\n",
    "from torchvision import transforms, datasets\n",
    "import argparse\n",
    "import matplotlib\n",
    "from src.Bayes_By_Backprop.model import *\n",
    "from src.Bayes_By_Backprop_Local_Reparametrization.model import *\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from numpy.random import gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94c72d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "image_trans_size = 64\n",
    "batch_size = 20\n",
    "nb_epochs = 200\n",
    "\n",
    "pSGLD = False\n",
    "save_data = True\n",
    "n_samples = 50\n",
    "\n",
    "sample_freq = 2\n",
    "burn_in = 1000\n",
    "\n",
    "\n",
    "\n",
    "prior_sig = 0.1\n",
    "# Where to save models weights\n",
    "models_dir = 'models_BBB_COVID150'\n",
    "# Where to save plots and error, accuracy vectors\n",
    "results_dir = 'results_BBB_COVID150'\n",
    "\n",
    "lr = 0.0001\n",
    "\n",
    "model= 'Gaussian_prior'\n",
    "nsamples = int(n_samples)\n",
    "\n",
    "\n",
    "## weight saving parameters #######\n",
    "\n",
    "\n",
    "N_saves = 100\n",
    "resample_its = 50\n",
    "resample_prior_its = 15\n",
    "re_burn = 1e8\n",
    "###################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## weight saving parameters #######\n",
    "\n",
    "sim_steps = 2\n",
    "\n",
    "###################################\n",
    "\n",
    "Nsamples = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083e9752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80e4be04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_object(filename):\n",
    "    with open(filename, 'rb') as input:\n",
    "        return pickle.load(input)\n",
    "\n",
    "\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def mkdir(paths):\n",
    "    if not isinstance(paths, (list, tuple)):\n",
    "        paths = [paths]\n",
    "    for path in paths:\n",
    "        if not os.path.isdir(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "\n",
    "suffixes = ['B', 'KB', 'MB', 'GB', 'TB', 'PB']\n",
    "\n",
    "\n",
    "def humansize(nbytes):\n",
    "    i = 0\n",
    "    while nbytes >= 1024 and i < len(suffixes) - 1:\n",
    "        nbytes /= 1024.\n",
    "        i += 1\n",
    "    f = ('%.2f' % nbytes)\n",
    "    return '%s%s' % (f, suffixes[i])\n",
    "\n",
    "\n",
    "def get_num_batches(nb_samples, batch_size, roundup=True):\n",
    "    if roundup:\n",
    "        return ((nb_samples + (-nb_samples % batch_size)) / batch_size)  # roundup division\n",
    "    else:\n",
    "        return nb_samples / batch_size\n",
    "\n",
    "\n",
    "def generate_ind_batch(nb_samples, batch_size, random=True, roundup=True):\n",
    "    if random:\n",
    "        ind = np.random.permutation(nb_samples)\n",
    "    else:\n",
    "        ind = range(int(nb_samples))\n",
    "    for i in range(int(get_num_batches(nb_samples, batch_size, roundup))):\n",
    "        yield ind[i * batch_size: (i + 1) * batch_size]\n",
    "\n",
    "\n",
    "def to_variable(var=(), cuda=True, volatile=False):\n",
    "    out = []\n",
    "    for v in var:\n",
    "        if isinstance(v, np.ndarray):\n",
    "            v = torch.from_numpy(v).type(torch.FloatTensor)\n",
    "\n",
    "        if not v.is_cuda and cuda:\n",
    "            v = v.cuda()\n",
    "\n",
    "        if not isinstance(v, Variable):\n",
    "            v = Variable(v, volatile=volatile)\n",
    "\n",
    "        out.append(v)\n",
    "    return out\n",
    "\n",
    "\n",
    "def cprint(color, text, **kwargs):\n",
    "    if color[0] == '*':\n",
    "        pre_code = '1;'\n",
    "        color = color[1:]\n",
    "    else:\n",
    "        pre_code = ''\n",
    "    code = {\n",
    "        'a': '30',\n",
    "        'r': '31',\n",
    "        'g': '32',\n",
    "        'y': '33',\n",
    "        'b': '34',\n",
    "        'p': '35',\n",
    "        'c': '36',\n",
    "        'w': '37'\n",
    "    }\n",
    "    print(\"\\x1b[%s%sm%s\\x1b[0m\" % (pre_code, code[color], text), **kwargs)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "def shuffle_in_unison_scary(a, b):\n",
    "    rng_state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(b)\n",
    "\n",
    "\n",
    "class Datafeed(data.Dataset):\n",
    "\n",
    "    def __init__(self, x_train, y_train, transform=None):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.x_train[index]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, self.y_train[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_train)\n",
    "\n",
    "class DatafeedImage(data.Dataset):\n",
    "    def __init__(self, x_train, y_train, transform=None):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.x_train[index]\n",
    "        img = Image.fromarray(np.uint8(img))\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, self.y_train[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_train)\n",
    "\n",
    "\n",
    "### functions for BNN with gauss output: ###\n",
    "\n",
    "def diagonal_gauss_loglike(x, mu, sigma):\n",
    "    # note that we can just treat each dim as isotropic and then do sum\n",
    "    cte_term = -(0.5)*np.log(2*np.pi)\n",
    "    det_sig_term = -torch.log(sigma)\n",
    "    inner = (x - mu)/sigma\n",
    "    dist_term = -(0.5)*(inner**2)\n",
    "    log_px = (cte_term + det_sig_term + dist_term).sum(dim=1, keepdim=False)\n",
    "    return log_px\n",
    "\n",
    "def get_rms(mu, y, y_means, y_stds):\n",
    "    x_un = mu * y_stds + y_means\n",
    "    y_un = y * y_stds + y_means\n",
    "    return torch.sqrt(((x_un - y_un)**2).sum() / y.shape[0])\n",
    "\n",
    "\n",
    "def get_loglike(mu, sigma, y, y_means, y_stds):\n",
    "    mu_un = mu * y_stds + y_means\n",
    "    y_un = y * y_stds + y_means\n",
    "    sigma_un = sigma * y_stds\n",
    "    ll = diagonal_gauss_loglike(y_un, mu_un, sigma_un)\n",
    "    return ll.mean(dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd1b5e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BaseNet(object):\n",
    "    def __init__(self):\n",
    "        cprint('c', '\\nNet:')\n",
    "\n",
    "    def get_nb_parameters(self):\n",
    "        return np.sum(p.numel() for p in self.model.parameters())\n",
    "\n",
    "    def set_mode_train(self, train=True):\n",
    "        if train:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "\n",
    "    def update_lr(self, epoch, gamma=0.99):\n",
    "        self.epoch += 1\n",
    "        if self.schedule is not None:\n",
    "            if len(self.schedule) == 0 or epoch in self.schedule:\n",
    "                self.lr *= gamma\n",
    "                print('learning rate: %f  (%d)\\n' % self.lr, epoch)\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr'] = self.lr\n",
    "\n",
    "    def save(self, filename):\n",
    "        cprint('c', 'Writting %s\\n' % filename)\n",
    "        torch.save({\n",
    "            'epoch': self.epoch,\n",
    "            'lr': self.lr,\n",
    "            'model': self.model,\n",
    "            'optimizer': self.optimizer}, filename)\n",
    "\n",
    "    def load(self, filename):\n",
    "        cprint('c', 'Reading %s\\n' % filename)\n",
    "        state_dict = torch.load(filename)\n",
    "        self.epoch = state_dict['epoch']\n",
    "        self.lr = state_dict['lr']\n",
    "        self.model = state_dict['model']\n",
    "        self.optimizer = state_dict['optimizer']\n",
    "        print('  restoring epoch: %d, lr: %f' % (self.epoch, self.lr))\n",
    "        return self.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48c03fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class H_SA_SGHMC(Optimizer):\n",
    "    \"\"\" Stochastic Gradient Hamiltonian Monte-Carlo Sampler that uses scale adaption during burn-in\n",
    "        procedure to find some hyperparamters. A gaussian prior is placed over parameters and a Gamma\n",
    "        Hyperprior is placed over the prior's standard deviation\"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1e-2, base_C=0.05, gauss_sig=0.1, alpha0=10, beta0=10):\n",
    "\n",
    "        self.eps = 1e-6\n",
    "        self.alpha0 = alpha0\n",
    "        self.beta0 = beta0\n",
    "\n",
    "        if gauss_sig == 0:\n",
    "            self.weight_decay = 0\n",
    "        else:\n",
    "            self.weight_decay = 1 / (gauss_sig ** 2)\n",
    "\n",
    "        if self.weight_decay <= 0.0:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if base_C < 0:\n",
    "            raise ValueError(\"Invalid friction term: {}\".format(base_C))\n",
    "\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            base_C=base_C,\n",
    "        )\n",
    "        super(H_SA_SGHMC, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, burn_in=False, resample_momentum=False, resample_prior=False):\n",
    "        \"\"\"Simulate discretized Hamiltonian dynamics for one step\"\"\"\n",
    "        loss = None\n",
    "\n",
    "        for group in self.param_groups:  # iterate over blocks -> the ones defined in defaults. We dont use groups.\n",
    "            for p in group[\"params\"]:  # these are weight and bias matrices\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                state = self.state[p]  # define dict for each individual param\n",
    "                if len(state) == 0:\n",
    "                    state[\"iteration\"] = 0\n",
    "                    state[\"tau\"] = torch.ones_like(p)\n",
    "                    state[\"g\"] = torch.ones_like(p)\n",
    "                    state[\"V_hat\"] = torch.ones_like(p)\n",
    "                    state[\"v_momentum\"] = torch.zeros_like(\n",
    "                        p)  # p.data.new(p.data.size()).normal_(mean=0, std=np.sqrt(group[\"lr\"])) #\n",
    "                    state['weight_decay'] = self.weight_decay\n",
    "\n",
    "                state[\"iteration\"] += 1  # this is kind of useless now but lets keep it provisionally\n",
    "\n",
    "                if resample_prior:\n",
    "                    alpha = self.alpha0 + p.data.nelement() / 2\n",
    "                    beta = self.beta0 + (p.data ** 2).sum().item() / 2\n",
    "                    gamma_sample = gamma(shape=alpha, scale=1 / (beta), size=None)\n",
    "                    #                     print('std', 1/np.sqrt(gamma_sample))\n",
    "                    state['weight_decay'] = gamma_sample\n",
    "\n",
    "                base_C, lr = group[\"base_C\"], group[\"lr\"]\n",
    "                weight_decay = state[\"weight_decay\"]\n",
    "                tau, g, V_hat = state[\"tau\"], state[\"g\"], state[\"V_hat\"]\n",
    "\n",
    "                d_p = p.grad.data\n",
    "                if weight_decay != 0:\n",
    "                    d_p.add_(weight_decay, p.data)\n",
    "\n",
    "                # update parameters during burn-in\n",
    "                if burn_in:  # We update g first as it makes most sense\n",
    "                    tau.add_(-tau * (g ** 2) / (\n",
    "                                V_hat + self.eps) + 1)  # specifies the moving average window, see Eq 9 in [1] left\n",
    "                    tau_inv = 1. / (tau + self.eps)\n",
    "                    g.add_(-tau_inv * g + tau_inv * d_p)  # average gradient see Eq 9 in [1] right\n",
    "                    V_hat.add_(-tau_inv * V_hat + tau_inv * (d_p ** 2))  # gradient variance see Eq 8 in [1]\n",
    "\n",
    "                V_sqrt = torch.sqrt(V_hat)\n",
    "                V_inv_sqrt = 1. / (V_sqrt + self.eps)  # preconditioner\n",
    "\n",
    "                if resample_momentum:  # equivalent to var = M under momentum reparametrisation\n",
    "                    state[\"v_momentum\"] = torch.normal(mean=torch.zeros_like(d_p),\n",
    "                                                       std=torch.sqrt((lr ** 2) * V_inv_sqrt))\n",
    "                v_momentum = state[\"v_momentum\"]\n",
    "\n",
    "                noise_var = (2. * (lr ** 2) * V_inv_sqrt * base_C - (lr ** 4))\n",
    "                noise_std = torch.sqrt(torch.clamp(noise_var, min=1e-16))\n",
    "                # sample random epsilon\n",
    "                noise_sample = torch.normal(mean=torch.zeros_like(d_p), std=torch.ones_like(d_p) * noise_std)\n",
    "\n",
    "                # update momentum (Eq 10 right in [1])\n",
    "                v_momentum.add_(- (lr ** 2) * V_inv_sqrt * d_p - base_C * v_momentum + noise_sample)\n",
    "\n",
    "                # update theta (Eq 10 left in [1])\n",
    "                p.data.add_(v_momentum)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9db43578",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, width, depth, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "\n",
    "        layers = [nn.Linear(input_dim, width), nn.ReLU()]\n",
    "        for i in range(depth - 1):\n",
    "            layers.append(nn.Linear(width, width))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(width, output_dim))\n",
    "\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class BNN_cat(BaseNet):  # for categorical distributions\n",
    "    def __init__(self, channels_in=1, side_in=224, classes=2, N_train = 300, lr=1e-2, cuda=True, grad_std_mul=30):\n",
    "        super(BNN_cat, self).__init__()\n",
    "\n",
    "        cprint('y', 'BNN categorical output')\n",
    "        self.lr = lr\n",
    "        self.channels_in = channels_in\n",
    "        self.side_in = side_in\n",
    "        self.classes = classes\n",
    "        self.model = MLP(input_dim=self.channels_in * self.side_in * self.side_in, width=1200, depth=2, output_dim=self.classes)\n",
    "        self.cuda = cuda\n",
    "\n",
    "        self.N_train = N_train\n",
    "        self.create_net()\n",
    "        self.create_opt()\n",
    "        self.schedule = None  # [] #[50,200,400,600]\n",
    "        self.epoch = 0\n",
    "\n",
    "        self.grad_buff = []\n",
    "        self.max_grad = 1e20\n",
    "        self.grad_std_mul = grad_std_mul\n",
    "\n",
    "        self.weight_set_samples = []\n",
    "\n",
    "    def create_net(self):\n",
    "        torch.manual_seed(42)\n",
    "        if self.cuda:\n",
    "            torch.cuda.manual_seed(42)\n",
    "        if self.cuda:\n",
    "            self.model.cuda()\n",
    "\n",
    "        print('    Total params: %.2fM' % (self.get_nb_parameters() / 1000000.0))\n",
    "\n",
    "    def create_opt(self):\n",
    "        \"\"\"This optimiser incorporates the gaussian prior term automatically. The prior variance is gibbs sampled from\n",
    "        its posterior using a gamma hyper-prior.\"\"\"\n",
    "        self.optimizer = H_SA_SGHMC(params=self.model.parameters(), lr=self.lr, base_C=0.05, gauss_sig=0.1)  # this last parameter does nothing\n",
    "\n",
    "    def fit(self, x, y, burn_in=False, resample_momentum=False, resample_prior=False):\n",
    "        self.set_mode_train(train=True)\n",
    "        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n",
    "        self.optimizer.zero_grad()\n",
    "        out = self.model(x)\n",
    "        loss = F.cross_entropy(out, y, reduction='mean')\n",
    "        loss = loss * self.N_train  # We use mean because we treat as an estimation of whole dataset\n",
    "        loss.backward()\n",
    "        #print('len', len(self.grad_buff))\n",
    "        #print([self.grad_buff[i].cpu() for i in range(len(self.grad_buff))])\n",
    "        # Gradient buffer to allow for dynamic clipping and prevent explosions\n",
    "        if len(self.grad_buff) > 1000:\n",
    "            #self.grad_buff = [self.grad_buff[i].cpu() for i in range(len(self.grad_buff))]\n",
    "            #print('len',len(self.grad_buff), self.grad_buff)\n",
    "            #print(type(self.grad_buff))\n",
    "            #print(np.array(self.grad_buff))\n",
    "\n",
    "            self.max_grad = np.mean([self.grad_buff[i].cpu() for i in range(len(self.grad_buff))]) + self.grad_std_mul * np.std([self.grad_buff[i].cpu() for i in range(len(self.grad_buff))])\n",
    "            self.grad_buff.pop(0)\n",
    "        # Clipping to prevent explosions\n",
    "        self.grad_buff.append(nn.utils.clip_grad_norm_(parameters=self.model.parameters(),\n",
    "                                                       max_norm=self.max_grad, norm_type=2))\n",
    "        if self.grad_buff[-1] >= self.max_grad:\n",
    "            print(self.max_grad, self.grad_buff[-1])\n",
    "            self.grad_buff.pop()\n",
    "        self.optimizer.step(burn_in=burn_in, resample_momentum=resample_momentum, resample_prior=resample_prior)\n",
    "\n",
    "        # out: (batch_size, out_channels, out_caps_dims)\n",
    "        pred = out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n",
    "        err = pred.ne(y.data).sum()\n",
    "\n",
    "        return loss.data * x.shape[0] / self.N_train, err\n",
    "\n",
    "    def eval(self, x, y, train=False):\n",
    "        self.set_mode_train(train=False)\n",
    "        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n",
    "\n",
    "        out = self.model(x)\n",
    "        loss = F.cross_entropy(out, y, reduction='sum')\n",
    "        probs = F.softmax(out, dim=1).data.cpu()\n",
    "\n",
    "        pred = out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n",
    "        err = pred.ne(y.data).sum()\n",
    "\n",
    "        return loss.data, err, probs\n",
    "\n",
    "    def save_sampled_net(self, max_samples):\n",
    "\n",
    "        if len(self.weight_set_samples) >= max_samples:\n",
    "            self.weight_set_samples.pop(0)\n",
    "\n",
    "        self.weight_set_samples.append(copy.deepcopy(self.model.state_dict()))\n",
    "\n",
    "        cprint('c', ' saving weight samples %d/%d' % (len(self.weight_set_samples), max_samples))\n",
    "        return None\n",
    "\n",
    "    def predict(self, x):\n",
    "        self.set_mode_train(train=False)\n",
    "        x, = to_variable(var=(x, ), cuda=self.cuda)\n",
    "        out = self.model(x)\n",
    "        probs = F.softmax(out, dim=1).data.cpu()\n",
    "        return probs.data\n",
    "\n",
    "    def sample_predict(self, x, Nsamples=0, grad=False):\n",
    "        \"\"\"return predictions using multiple samples from posterior\"\"\"\n",
    "        self.set_mode_train(train=False)\n",
    "        if Nsamples == 0:\n",
    "            Nsamples = len(self.weight_set_samples)\n",
    "        x, = to_variable(var=(x, ), cuda=self.cuda)\n",
    "\n",
    "        if grad:\n",
    "            self.optimizer.zero_grad()\n",
    "            if not x.requires_grad:\n",
    "                x.requires_grad = True\n",
    "\n",
    "        out = x.data.new(Nsamples, x.shape[0], self.classes)\n",
    "\n",
    "        # iterate over all saved weight configuration samples\n",
    "        for idx, weight_dict in enumerate(self.weight_set_samples):\n",
    "            if idx == Nsamples:\n",
    "                break\n",
    "            self.model.load_state_dict(weight_dict)\n",
    "            out[idx] = self.model(x)\n",
    "\n",
    "        out = out[:idx]\n",
    "        prob_out = F.softmax(out, dim=2)\n",
    "\n",
    "        if grad:\n",
    "            return prob_out\n",
    "        else:\n",
    "            return prob_out.data\n",
    "\n",
    "    def sample_eval(self, x, y, Nsamples=0, grad=False):\n",
    "        \"\"\"return predictions using multiple samples from posterior\"\"\"\n",
    "        self.set_mode_train(train=False)\n",
    "        if Nsamples == 0:\n",
    "            Nsamples = len(self.weight_set_samples)\n",
    "        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n",
    "\n",
    "        if grad:\n",
    "            self.optimizer.zero_grad()\n",
    "            if not x.requires_grad:\n",
    "                x.requires_grad = True\n",
    "\n",
    "        out = x.data.new(Nsamples, x.shape[0], self.classes)\n",
    "        #print('momomomomo', len(self.weight_set_samples))\n",
    "        # iterate over all saved weight configuration samples\n",
    "        for idx, weight_dict in enumerate(self.weight_set_samples):\n",
    "            #print('iuiiii', idx)\n",
    "            if idx == Nsamples:\n",
    "                break\n",
    "            self.model.load_state_dict(weight_dict)\n",
    "            #print('ccccccc', self.model, x.size()[0])\n",
    "            #print('mmmmmmm', x.resize_(x.size()[0], self.side_in * self.side_in).size())\n",
    "            out[idx] = self.model(x.resize_(x.size()[0], self.side_in * self.side_in))\n",
    "\n",
    "        #print('idxxxxxxxxxx', idx)\n",
    "        out = out[:idx]\n",
    "\n",
    "        mean_out = F.softmax(out, dim=2).mean(dim=0, keepdim=False)\n",
    "\n",
    "        loss = F.cross_entropy(mean_out, y, reduction='sum')\n",
    "        prob_out = F.softmax(mean_out, dim=1).data.cpu()\n",
    "\n",
    "        pred = mean_out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n",
    "        err = pred.ne(y.data).sum()\n",
    "\n",
    "        if grad:\n",
    "            return loss.data, err, prob_out\n",
    "        else:\n",
    "            return loss.data, err, prob_out\n",
    "\n",
    "    def get_weight_samples(self, Nsamples=0):\n",
    "        \"\"\"return weight samples from posterior in a single-column array\"\"\"\n",
    "        weight_vec = []\n",
    "\n",
    "        if Nsamples == 0 or Nsamples > len(self.weight_set_samples):\n",
    "            Nsamples = len(self.weight_set_samples)\n",
    "\n",
    "        for idx, state_dict in enumerate(self.weight_set_samples):\n",
    "            if idx == Nsamples:\n",
    "                break\n",
    "\n",
    "            for key in state_dict.keys():\n",
    "                if 'weight' in key:\n",
    "                    weight_mtx = state_dict[key].cpu().data\n",
    "                    for weight in weight_mtx.view(-1):\n",
    "                        weight_vec.append(weight)\n",
    "\n",
    "        return np.array(weight_vec)\n",
    "\n",
    "    def save_weights(self, filename):\n",
    "        save_object(self.weight_set_samples, filename)\n",
    "\n",
    "    def load_weights(self, filename, subsample=1):\n",
    "        self.weight_set_samples = load_object(filename)\n",
    "        self.weight_set_samples = self.weight_set_samples[::subsample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39caf646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658bf685",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "240f8f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\n",
      "Data:\u001b[0m\n",
      "\u001b[36m\n",
      "Data:\u001b[0m\n",
      "\u001b[36m\n",
      "Network:\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "transform_covid19 = transforms.Compose([\n",
    "    transforms.Resize(image_trans_size),\n",
    "    transforms.CenterCrop(image_trans_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.Grayscale(num_output_channels=1)\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(root=\"./notebooks/data/COVID/train\", transform=transform_covid19)\n",
    "valset = torchvision.datasets.ImageFolder(root=\"./notebooks/data/COVID/test\", transform=transform_covid19)\n",
    "classes = 2\n",
    "train_data_len = len(trainset.targets)\n",
    "test_data_len = len(valset.targets)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "NTrainPoints = train_data_len\n",
    "\n",
    "\n",
    "\n",
    "mkdir(models_dir)\n",
    "mkdir(results_dir)\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# train config\n",
    "\n",
    "\n",
    "log_interval = 1\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# dataset\n",
    "cprint('c', '\\nData:')\n",
    "\n",
    "nb_its_dev = log_interval\n",
    "flat_ims = True\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# dataset\n",
    "cprint('c', '\\nData:')\n",
    "\n",
    "# load data\n",
    "\n",
    "# data augmentation\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True,\n",
    "                                              num_workers=0)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True,\n",
    "                                            num_workers=0)\n",
    "\n",
    "else:\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=False,\n",
    "                                              num_workers=0)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n",
    "                                            num_workers=0)\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# net dims\n",
    "cprint('c', '\\nNetwork:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2d4a14",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\n",
      "Net:\u001b[0m\n",
      "\u001b[33mBNN categorical output\u001b[0m\n",
      "    Total params: 6.36M\n",
      "\u001b[36m\n",
      "Train:\u001b[0m\n",
      "  init cost variables:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHUAIZ~1\\AppData\\Local\\Temp/ipykernel_30804/272664775.py:6: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return np.sum(p.numel() for p in self.model.parameters())\n",
      "C:\\Users\\SHUAIZ~1\\AppData\\Local\\Temp/ipykernel_30804/1161784216.py:63: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:1050.)\n",
      "  d_p.add_(weight_decay, p.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 0/200, Jtr_pred = 0.699913, err = 0.561000, \u001b[31m   time: 175.818824 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.698684, err = 0.548333\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "it 1/200, Jtr_pred = 0.699366, err = 0.558000, \u001b[31m   time: 177.387989 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.698256, err = 0.528333\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "it 2/200, Jtr_pred = 0.699604, err = 0.558000, \u001b[31m   time: 177.895211 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.698369, err = 0.516667\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "it 3/200, Jtr_pred = 0.700131, err = 0.557000, \u001b[31m   time: 210.654623 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.698906, err = 0.533333\n",
      "\u001b[0m\n",
      "it 4/200, Jtr_pred = 0.700829, err = 0.555000, \u001b[31m   time: 238.948646 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.698951, err = 0.510000\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "it 5/200, Jtr_pred = 0.701217, err = 0.552500, \u001b[31m   time: 188.005286 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.698895, err = 0.516667\n",
      "\u001b[0m\n",
      "it 6/200, Jtr_pred = 0.701002, err = 0.551000, \u001b[31m   time: 185.946516 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.698778, err = 0.535000\n",
      "\u001b[0m\n",
      "it 7/200, Jtr_pred = 0.700691, err = 0.557500, \u001b[31m   time: 187.944967 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.698507, err = 0.536667\n",
      "\u001b[0m\n",
      "it 8/200, Jtr_pred = 0.699516, err = 0.553500, \u001b[31m   time: 186.462532 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.697917, err = 0.531667\n",
      "\u001b[0m\n",
      "it 9/200, Jtr_pred = 0.699149, err = 0.543000, \u001b[31m   time: 186.059125 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.698111, err = 0.521667\n",
      "\u001b[0m\n",
      "it 10/200, Jtr_pred = 0.698977, err = 0.535500, \u001b[31m   time: 200.144393 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.697262, err = 0.508333\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "it 11/200, Jtr_pred = 0.699109, err = 0.532500, \u001b[31m   time: 200.531271 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.698609, err = 0.521667\n",
      "\u001b[0m\n",
      "it 12/200, Jtr_pred = 0.699229, err = 0.542500, \u001b[31m   time: 196.464597 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.698731, err = 0.525000\n",
      "\u001b[0m\n",
      "it 13/200, Jtr_pred = 0.698753, err = 0.543000, \u001b[31m   time: 187.098057 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.698679, err = 0.543333\n",
      "\u001b[0m\n",
      "it 14/200, Jtr_pred = 0.698261, err = 0.537500, \u001b[31m   time: 187.617689 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.697476, err = 0.520000\n",
      "\u001b[0m\n",
      "it 15/200, Jtr_pred = 0.697804, err = 0.522000, \u001b[31m   time: 190.530268 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.696344, err = 0.503333\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "it 16/200, Jtr_pred = 0.697627, err = 0.524500, \u001b[31m   time: 190.267932 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.697066, err = 0.530000\n",
      "\u001b[0m\n",
      "it 17/200, Jtr_pred = 0.697563, err = 0.535500, \u001b[31m   time: 189.976923 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.696788, err = 0.500000\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "it 18/200, Jtr_pred = 0.697712, err = 0.539000, \u001b[31m   time: 190.664108 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.697213, err = 0.513333\n",
      "\u001b[0m\n",
      "it 19/200, Jtr_pred = 0.697729, err = 0.537500, \u001b[31m   time: 189.594659 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.696979, err = 0.531667\n",
      "\u001b[0m\n",
      "it 20/200, Jtr_pred = 0.697406, err = 0.536500, \u001b[31m   time: 192.349347 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.696719, err = 0.530000\n",
      "\u001b[0m\n",
      "it 21/200, Jtr_pred = 0.696986, err = 0.540500, \u001b[31m   time: 198.108548 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.696211, err = 0.528333\n",
      "\u001b[0m\n",
      "it 22/200, Jtr_pred = 0.696048, err = 0.528000, \u001b[31m   time: 205.338120 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.694775, err = 0.501667\n",
      "\u001b[0m\n",
      "it 23/200, Jtr_pred = 0.695149, err = 0.523000, \u001b[31m   time: 206.106117 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.694585, err = 0.498333\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "it 24/200, Jtr_pred = 0.695038, err = 0.522000, \u001b[31m   time: 203.314868 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.694026, err = 0.486667\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "it 25/200, Jtr_pred = 0.694776, err = 0.509000, \u001b[31m   time: 203.777501 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.693559, err = 0.483333\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "it 26/200, Jtr_pred = 0.694558, err = 0.512500, \u001b[31m   time: 199.616710 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.693658, err = 0.481667\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "it 27/200, Jtr_pred = 0.694641, err = 0.516000, \u001b[31m   time: 198.617292 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.693891, err = 0.485000\n",
      "\u001b[0m\n",
      "it 28/200, Jtr_pred = 0.694421, err = 0.515000, \u001b[31m   time: 198.760716 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.693909, err = 0.485000\n",
      "\u001b[0m\n",
      "it 29/200, Jtr_pred = 0.694394, err = 0.506000, \u001b[31m   time: 200.308136 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.694661, err = 0.498333\n",
      "\u001b[0m\n",
      "it 30/200, Jtr_pred = 0.694614, err = 0.511500, \u001b[31m   time: 197.551088 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.694620, err = 0.501667\n",
      "\u001b[0m\n",
      "it 31/200, Jtr_pred = 0.694416, err = 0.501500, \u001b[31m   time: 197.014693 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.694470, err = 0.496667\n",
      "\u001b[0m\n",
      "it 32/200, Jtr_pred = 0.694357, err = 0.498000, \u001b[31m   time: 195.606520 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.694226, err = 0.490000\n",
      "\u001b[0m\n",
      "it 33/200, Jtr_pred = 0.694288, err = 0.501000, \u001b[31m   time: 196.631610 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.694338, err = 0.483333\n",
      "\u001b[0m\n",
      "it 34/200, Jtr_pred = 0.694233, err = 0.497000, \u001b[31m   time: 195.528908 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.694201, err = 0.500000\n",
      "\u001b[0m\n",
      "it 35/200, Jtr_pred = 0.694428, err = 0.507500, \u001b[31m   time: 197.886149 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.694419, err = 0.506667\n",
      "\u001b[0m\n",
      "it 36/200, Jtr_pred = 0.694776, err = 0.511500, \u001b[31m   time: 197.240768 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.694194, err = 0.510000\n",
      "\u001b[0m\n",
      "it 37/200, Jtr_pred = 0.694538, err = 0.503000, \u001b[31m   time: 196.436801 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.693614, err = 0.485000\n",
      "\u001b[0m\n",
      "it 38/200, Jtr_pred = 0.694125, err = 0.497000, \u001b[31m   time: 197.111677 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.693225, err = 0.493333\n",
      "\u001b[0m\n",
      "it 39/200, Jtr_pred = 0.694292, err = 0.503500, \u001b[31m   time: 198.769210 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.692157, err = 0.481667\n",
      "\u001b[0m\n",
      "it 40/200, Jtr_pred = 0.694034, err = 0.497000, \u001b[31m   time: 200.118181 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.691942, err = 0.478333\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "it 41/200, Jtr_pred = 0.693963, err = 0.502000, \u001b[31m   time: 193.235890 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.691474, err = 0.448333\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "it 42/200, Jtr_pred = 0.693923, err = 0.497500, \u001b[31m   time: 197.778938 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.692273, err = 0.471667\n",
      "\u001b[0m\n",
      "it 43/200, Jtr_pred = 0.694004, err = 0.495500, \u001b[31m   time: 194.977097 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.692464, err = 0.468333\n",
      "\u001b[0m\n",
      "it 44/200, Jtr_pred = 0.694151, err = 0.494500, \u001b[31m   time: 193.092371 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.692263, err = 0.466667\n",
      "\u001b[0m\n",
      "it 45/200, Jtr_pred = 0.694002, err = 0.491000, \u001b[31m   time: 203.161551 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.691711, err = 0.455000\n",
      "\u001b[0m\n",
      "it 46/200, Jtr_pred = 0.693792, err = 0.483000, \u001b[31m   time: 196.493840 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.691222, err = 0.463333\n",
      "\u001b[0m\n",
      "it 47/200, Jtr_pred = 0.693499, err = 0.491500, \u001b[31m   time: 192.044377 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.691249, err = 0.453333\n",
      "\u001b[0m\n",
      "it 48/200, Jtr_pred = 0.693668, err = 0.482500, \u001b[31m   time: 191.628510 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.692433, err = 0.455000\n",
      "\u001b[0m\n",
      "it 49/200, Jtr_pred = 0.693888, err = 0.487000, \u001b[31m   time: 192.272517 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.692084, err = 0.455000\n",
      "\u001b[0m\n",
      "it 50/200, Jtr_pred = 0.693464, err = 0.487000, \u001b[31m   time: 191.320678 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.692090, err = 0.468333\n",
      "\u001b[0m\n",
      "it 51/200, Jtr_pred = 0.693313, err = 0.488000, \u001b[31m   time: 197.888040 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.692072, err = 0.458333\n",
      "\u001b[0m\n",
      "it 52/200, Jtr_pred = 0.693388, err = 0.493000, \u001b[31m   time: 196.474013 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.691250, err = 0.460000\n",
      "\u001b[0m\n",
      "it 53/200, Jtr_pred = 0.692798, err = 0.479500, \u001b[31m   time: 195.946198 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.691083, err = 0.458333\n",
      "\u001b[0m\n",
      "it 54/200, Jtr_pred = 0.692723, err = 0.480500, \u001b[31m   time: 196.542565 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.690656, err = 0.460000\n",
      "\u001b[0m\n",
      "it 55/200, Jtr_pred = 0.692575, err = 0.490000, \u001b[31m   time: 196.235452 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.690590, err = 0.450000\n",
      "\u001b[0m\n",
      "it 56/200, Jtr_pred = 0.692312, err = 0.489000, \u001b[31m   time: 198.196767 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.689917, err = 0.446667\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "it 57/200, Jtr_pred = 0.692216, err = 0.482000, \u001b[31m   time: 195.404540 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.690142, err = 0.463333\n",
      "\u001b[0m\n",
      "it 58/200, Jtr_pred = 0.692271, err = 0.477000, \u001b[31m   time: 196.296475 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.690205, err = 0.455000\n",
      "\u001b[0m\n",
      "it 59/200, Jtr_pred = 0.692238, err = 0.479000, \u001b[31m   time: 198.580847 seconds\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m    Jdev = 0.689911, err = 0.463333\n",
      "\u001b[0m\n",
      "it 60/200, Jtr_pred = 0.692185, err = 0.481500, \u001b[31m   time: 199.108208 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.689072, err = 0.466667\n",
      "\u001b[0m\n",
      "it 61/200, Jtr_pred = 0.692120, err = 0.486500, \u001b[31m   time: 201.172250 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.689322, err = 0.466667\n",
      "\u001b[0m\n",
      "it 62/200, Jtr_pred = 0.692295, err = 0.485500, \u001b[31m   time: 208.596002 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.689733, err = 0.470000\n",
      "\u001b[0m\n",
      "it 63/200, Jtr_pred = 0.692139, err = 0.484000, \u001b[31m   time: 218.605281 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.689212, err = 0.470000\n",
      "\u001b[0m\n",
      "it 64/200, Jtr_pred = 0.692075, err = 0.485500, \u001b[31m   time: 204.739630 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.689179, err = 0.475000\n",
      "\u001b[0m\n",
      "it 65/200, Jtr_pred = 0.692120, err = 0.488000, \u001b[31m   time: 207.276512 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.688981, err = 0.480000\n",
      "\u001b[0m\n",
      "it 66/200, Jtr_pred = 0.692301, err = 0.490000, \u001b[31m   time: 195.419470 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.689400, err = 0.478333\n",
      "\u001b[0m\n",
      "it 67/200, Jtr_pred = 0.691566, err = 0.488000, \u001b[31m   time: 216.909742 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.688093, err = 0.460000\n",
      "\u001b[0m\n",
      "it 68/200, Jtr_pred = 0.690630, err = 0.483000, \u001b[31m   time: 201.679441 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.688154, err = 0.466667\n",
      "\u001b[0m\n",
      "it 69/200, Jtr_pred = 0.690391, err = 0.479500, \u001b[31m   time: 203.616942 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.686765, err = 0.465000\n",
      "\u001b[0m\n",
      "it 70/200, Jtr_pred = 0.689944, err = 0.477500, \u001b[31m   time: 199.060986 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.686285, err = 0.456667\n",
      "\u001b[0m\n",
      "it 71/200, Jtr_pred = 0.689222, err = 0.483500, \u001b[31m   time: 203.572592 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.685118, err = 0.458333\n",
      "\u001b[0m\n",
      "it 72/200, Jtr_pred = 0.689294, err = 0.469000, \u001b[31m   time: 220.165482 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.684766, err = 0.445000\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "it 73/200, Jtr_pred = 0.689013, err = 0.469000, \u001b[31m   time: 221.486724 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.684267, err = 0.435000\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "it 74/200, Jtr_pred = 0.688471, err = 0.461500, \u001b[31m   time: 216.917089 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.684196, err = 0.426667\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "it 75/200, Jtr_pred = 0.687917, err = 0.457000, \u001b[31m   time: 203.157479 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.683522, err = 0.405000\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "it 76/200, Jtr_pred = 0.687486, err = 0.459500, \u001b[31m   time: 207.834783 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.683511, err = 0.433333\n",
      "\u001b[0m\n",
      "it 77/200, Jtr_pred = 0.687304, err = 0.457000, \u001b[31m   time: 213.284795 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.683396, err = 0.421667\n",
      "\u001b[0m\n",
      "it 78/200, Jtr_pred = 0.687512, err = 0.450500, \u001b[31m   time: 223.383969 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.683179, err = 0.418333\n",
      "\u001b[0m\n",
      "it 79/200, Jtr_pred = 0.687770, err = 0.458500, \u001b[31m   time: 224.801803 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.683782, err = 0.426667\n",
      "\u001b[0m\n",
      "it 80/200, Jtr_pred = 0.687225, err = 0.460000, \u001b[31m   time: 222.164737 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.682760, err = 0.416667\n",
      "\u001b[0m\n",
      "it 81/200, Jtr_pred = 0.686382, err = 0.456500, \u001b[31m   time: 219.700242 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.682766, err = 0.433333\n",
      "\u001b[0m\n",
      "it 82/200, Jtr_pred = 0.686585, err = 0.453000, \u001b[31m   time: 218.037400 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.683681, err = 0.435000\n",
      "\u001b[0m\n",
      "it 83/200, Jtr_pred = 0.686873, err = 0.453500, \u001b[31m   time: 217.201913 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.683587, err = 0.448333\n",
      "\u001b[0m\n",
      "it 84/200, Jtr_pred = 0.686736, err = 0.453000, \u001b[31m   time: 226.154821 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.683580, err = 0.440000\n",
      "\u001b[0m\n",
      "it 85/200, Jtr_pred = 0.686730, err = 0.455000, \u001b[31m   time: 211.914174 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.683666, err = 0.445000\n",
      "\u001b[0m\n",
      "it 86/200, Jtr_pred = 0.686269, err = 0.451500, \u001b[31m   time: 216.209108 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.684333, err = 0.440000\n",
      "\u001b[0m\n",
      "it 87/200, Jtr_pred = 0.686386, err = 0.439000, \u001b[31m   time: 205.365615 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.684305, err = 0.441667\n",
      "\u001b[0m\n",
      "it 88/200, Jtr_pred = 0.686459, err = 0.444000, \u001b[31m   time: 211.525595 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.684804, err = 0.448333\n",
      "\u001b[0m\n",
      "it 89/200, Jtr_pred = 0.686791, err = 0.451500, \u001b[31m   time: 201.275014 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.684999, err = 0.450000\n",
      "\u001b[0m\n",
      "it 90/200, Jtr_pred = 0.687001, err = 0.451500, \u001b[31m   time: 203.976430 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.685012, err = 0.456667\n",
      "\u001b[0m\n",
      "it 91/200, Jtr_pred = 0.687197, err = 0.450500, \u001b[31m   time: 208.888360 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.685103, err = 0.456667\n",
      "\u001b[0m\n",
      "it 92/200, Jtr_pred = 0.687260, err = 0.452500, \u001b[31m   time: 210.020731 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.684541, err = 0.448333\n",
      "\u001b[0m\n",
      "it 93/200, Jtr_pred = 0.686955, err = 0.448500, \u001b[31m   time: 213.450191 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.683644, err = 0.438333\n",
      "\u001b[0m\n",
      "it 94/200, Jtr_pred = 0.687498, err = 0.449500, \u001b[31m   time: 211.573968 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.684644, err = 0.443333\n",
      "\u001b[0m\n",
      "it 95/200, Jtr_pred = 0.687672, err = 0.448000, \u001b[31m   time: 207.777582 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.685355, err = 0.453333\n",
      "\u001b[0m\n",
      "it 96/200, Jtr_pred = 0.687482, err = 0.450000, \u001b[31m   time: 212.101081 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.684760, err = 0.440000\n",
      "\u001b[0m\n",
      "it 97/200, Jtr_pred = 0.687428, err = 0.455000, \u001b[31m   time: 200.616383 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.684321, err = 0.438333\n",
      "\u001b[0m\n",
      "it 98/200, Jtr_pred = 0.686856, err = 0.457000, \u001b[31m   time: 193.149406 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.683855, err = 0.440000\n",
      "\u001b[0m\n",
      "it 99/200, Jtr_pred = 0.687440, err = 0.460500, \u001b[31m   time: 206.125922 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.684508, err = 0.453333\n",
      "\u001b[0m\n",
      "it 100/200, Jtr_pred = 0.687510, err = 0.459500, \u001b[31m   time: 205.787660 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.684233, err = 0.440000\n",
      "\u001b[0m\n",
      "it 101/200, Jtr_pred = 0.687119, err = 0.460500, \u001b[31m   time: 194.436461 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.684822, err = 0.443333\n",
      "\u001b[0m\n",
      "it 102/200, Jtr_pred = 0.687466, err = 0.460500, \u001b[31m   time: 194.106858 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.685139, err = 0.458333\n",
      "\u001b[0m\n",
      "it 103/200, Jtr_pred = 0.688096, err = 0.462500, \u001b[31m   time: 190.271747 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.685782, err = 0.451667\n",
      "\u001b[0m\n",
      "it 104/200, Jtr_pred = 0.688975, err = 0.469000, \u001b[31m   time: 196.526906 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.686295, err = 0.460000\n",
      "\u001b[0m\n",
      "it 105/200, Jtr_pred = 0.689394, err = 0.479500, \u001b[31m   time: 198.129347 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.687254, err = 0.466667\n",
      "\u001b[0m\n",
      "it 106/200, Jtr_pred = 0.689797, err = 0.479000, \u001b[31m   time: 193.193518 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.688270, err = 0.478333\n",
      "\u001b[0m\n",
      "it 107/200, Jtr_pred = 0.689360, err = 0.475500, \u001b[31m   time: 193.484635 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.689211, err = 0.486667\n",
      "\u001b[0m\n",
      "it 108/200, Jtr_pred = 0.689431, err = 0.474500, \u001b[31m   time: 191.263324 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.690742, err = 0.491667\n",
      "\u001b[0m\n",
      "it 109/200, Jtr_pred = 0.689968, err = 0.479000, \u001b[31m   time: 190.371277 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.691043, err = 0.493333\n",
      "\u001b[0m\n",
      "it 110/200, Jtr_pred = 0.690228, err = 0.475500, \u001b[31m   time: 195.456763 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.691482, err = 0.490000\n",
      "\u001b[0m\n",
      "it 111/200, Jtr_pred = 0.690724, err = 0.480500, \u001b[31m   time: 202.057985 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.691228, err = 0.493333\n",
      "\u001b[0m\n",
      "it 112/200, Jtr_pred = 0.690448, err = 0.479500, \u001b[31m   time: 196.971544 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.691127, err = 0.500000\n",
      "\u001b[0m\n",
      "it 113/200, Jtr_pred = 0.690587, err = 0.479000, \u001b[31m   time: 200.619375 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.690651, err = 0.483333\n",
      "\u001b[0m\n",
      "it 114/200, Jtr_pred = 0.690343, err = 0.480000, \u001b[31m   time: 200.199476 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.690438, err = 0.488333\n",
      "\u001b[0m\n",
      "it 115/200, Jtr_pred = 0.690532, err = 0.474500, \u001b[31m   time: 197.136049 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.690197, err = 0.485000\n",
      "\u001b[0m\n",
      "it 116/200, Jtr_pred = 0.689959, err = 0.469000, \u001b[31m   time: 191.896199 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.689370, err = 0.481667\n",
      "\u001b[0m\n",
      "it 117/200, Jtr_pred = 0.689277, err = 0.465000, \u001b[31m   time: 191.730812 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.688553, err = 0.471667\n",
      "\u001b[0m\n",
      "it 118/200, Jtr_pred = 0.689543, err = 0.467000, \u001b[31m   time: 192.478414 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.687320, err = 0.470000\n",
      "\u001b[0m\n",
      "it 119/200, Jtr_pred = 0.689814, err = 0.468500, \u001b[31m   time: 193.240908 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.687807, err = 0.466667\n",
      "\u001b[0m\n",
      "it 120/200, Jtr_pred = 0.688695, err = 0.466500, \u001b[31m   time: 196.154664 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.686620, err = 0.463333\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 121/200, Jtr_pred = 0.688127, err = 0.465000, \u001b[31m   time: 192.513602 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.686364, err = 0.446667\n",
      "\u001b[0m\n",
      "it 122/200, Jtr_pred = 0.687778, err = 0.468000, \u001b[31m   time: 191.425279 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.684609, err = 0.453333\n",
      "\u001b[0m\n",
      "it 123/200, Jtr_pred = 0.687444, err = 0.465500, \u001b[31m   time: 194.010899 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.684532, err = 0.453333\n",
      "\u001b[0m\n",
      "it 124/200, Jtr_pred = 0.687495, err = 0.464000, \u001b[31m   time: 191.492750 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.683534, err = 0.443333\n",
      "\u001b[0m\n",
      "it 125/200, Jtr_pred = 0.687101, err = 0.463000, \u001b[31m   time: 191.386029 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.683206, err = 0.451667\n",
      "\u001b[0m\n",
      "it 126/200, Jtr_pred = 0.686880, err = 0.462500, \u001b[31m   time: 187.890888 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.681954, err = 0.443333\n",
      "\u001b[0m\n",
      "it 127/200, Jtr_pred = 0.685439, err = 0.459500, \u001b[31m   time: 189.488477 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.680669, err = 0.440000\n",
      "\u001b[0m\n",
      "it 128/200, Jtr_pred = 0.684880, err = 0.457000, \u001b[31m   time: 191.389622 seconds\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "\n",
    "net = BNN_cat(channels_in = 1, side_in = image_trans_size, classes = classes, N_train = NTrainPoints, lr = lr, cuda = use_cuda, grad_std_mul = 20)\n",
    "\n",
    "\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# net dims\n",
    "epoch = 0\n",
    "it_count = 0\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# train\n",
    "cprint('c', '\\nTrain:')\n",
    "\n",
    "print('  init cost variables:')\n",
    "cost_train = np.zeros(nb_epochs)\n",
    "err_train = np.zeros(nb_epochs)\n",
    "cost_dev = np.zeros(nb_epochs)\n",
    "err_dev = np.zeros(nb_epochs)\n",
    "best_cost = np.inf\n",
    "best_err = np.inf\n",
    "\n",
    "tic0 = time.time()\n",
    "for i in range(epoch, nb_epochs):\n",
    "    net.set_mode_train(True)\n",
    "    tic = time.time()\n",
    "    nb_samples = 0\n",
    "    for x, y in trainloader:\n",
    "\n",
    "        if flat_ims:\n",
    "            x = x.view(x.shape[0], -1)\n",
    "\n",
    "        cost_pred, err = net.fit(x, y, burn_in=(i % re_burn < burn_in),\n",
    "                                 resample_momentum=(it_count % resample_its == 0),\n",
    "                                 resample_prior=(it_count % resample_prior_its == 0))\n",
    "        it_count += 1\n",
    "        err_train[i] += err\n",
    "        cost_train[i] += cost_pred\n",
    "        nb_samples += len(x)\n",
    "\n",
    "    cost_train[i] /= nb_samples\n",
    "    err_train[i] /= nb_samples\n",
    "    toc = time.time()\n",
    "\n",
    "    # ---- print\n",
    "    print(\"it %d/%d, Jtr_pred = %f, err = %f, \" % (i, nb_epochs, cost_train[i], err_train[i]), end=\"\")\n",
    "    cprint('r', '   time: %f seconds\\n' % (toc - tic))\n",
    "    net.update_lr(i)\n",
    "\n",
    "    # ---- save weights\n",
    "    if i % re_burn >= burn_in and i % sim_steps == 0:\n",
    "        net.save_sampled_net(max_samples=N_saves)\n",
    "\n",
    "    # ---- dev\n",
    "    if i % nb_its_dev == 0:\n",
    "        nb_samples = 0\n",
    "        for j, (x, y) in enumerate(valloader):\n",
    "            if flat_ims:\n",
    "                x = x.view(x.shape[0], -1)\n",
    "\n",
    "            cost, err, probs = net.eval(x, y)\n",
    "\n",
    "            cost_dev[i] += cost\n",
    "            err_dev[i] += err\n",
    "            nb_samples += len(x)\n",
    "\n",
    "        cost_dev[i] /= nb_samples\n",
    "        err_dev[i] /= nb_samples\n",
    "\n",
    "        cprint('g', '    Jdev = %f, err = %f\\n' % (cost_dev[i], err_dev[i]))\n",
    "        if err_dev[i] < best_err:\n",
    "            best_err = err_dev[i]\n",
    "            cprint('b', 'best test error')\n",
    "\n",
    "toc0 = time.time()\n",
    "runtime_per_it = (toc0 - tic0) / float(nb_epochs)\n",
    "cprint('r', '   average time: %f seconds\\n' % runtime_per_it)\n",
    "\n",
    "## SAVE WEIGHTS\n",
    "net.save_weights(models_dir + '/state_dicts.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a76b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# fig cost vs its\n",
    "\n",
    "# fig cost vs its\n",
    "textsize = 15\n",
    "marker = 5\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(range(0, nb_epochs, nb_its_dev), np.clip(cost_dev[::nb_its_dev], a_min=-5, a_max=5), 'b-')\n",
    "ax1.plot(np.clip(cost_train, a_min=-5, a_max=5), 'r--')\n",
    "ax1.set_ylabel('Cross Entropy')\n",
    "plt.xlabel('epoch')\n",
    "plt.grid(b=True, which='major', color='k', linestyle='-')\n",
    "plt.grid(b=True, which='minor', color='k', linestyle='--')\n",
    "lgd = plt.legend(['test error', 'train error'], markerscale=marker, prop={'size': textsize, 'weight': 'normal'})\n",
    "ax = plt.gca()\n",
    "plt.title('classification costs')\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(textsize)\n",
    "    item.set_weight('normal')\n",
    "plt.savefig(results_dir + '/cost.png', bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.set_ylabel('% error')\n",
    "ax2.semilogy(range(0, nb_epochs, nb_its_dev), err_dev[::nb_its_dev], 'b-')\n",
    "ax2.semilogy(err_train, 'r--')\n",
    "ax2.set_ylim(top=1, bottom=1e-3)\n",
    "plt.xlabel('epoch')\n",
    "plt.grid(b=True, which='major', color='k', linestyle='-')\n",
    "plt.grid(b=True, which='minor', color='k', linestyle='--')\n",
    "ax2.get_yaxis().set_minor_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "ax2.get_yaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "lgd = plt.legend(['test error', 'train error'], markerscale=marker, prop={'size': textsize, 'weight': 'normal'})\n",
    "ax = plt.gca()\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(textsize)\n",
    "    item.set_weight('normal')\n",
    "plt.savefig(results_dir + '/err.png', bbox_extra_artists=(lgd,), box_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a319b99c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451cc05a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "735b3e68",
   "metadata": {},
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceb3511",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_samples = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937adc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_covid19 = transforms.Compose([\n",
    "    transforms.Resize(image_trans_size),\n",
    "    transforms.CenterCrop(image_trans_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.Grayscale(num_output_channels=1)\n",
    "])\n",
    "classes = 2\n",
    "trainset = torchvision.datasets.ImageFolder(root=\"./notebooks/data/COVID/train\", transform=transform_covid19)\n",
    "valset = torchvision.datasets.ImageFolder(root=\"./notebooks/data/COVID/test\", transform=transform_covid19)\n",
    "\n",
    "train_data_len = len(trainset.targets)\n",
    "test_data_len = len(valset.targets)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "NTrainPoints = train_data_len\n",
    "\n",
    "\n",
    "\n",
    "mkdir(models_dir)\n",
    "mkdir(results_dir)\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# train config\n",
    "\n",
    "log_interval = 1\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# dataset\n",
    "cprint('c', '\\nData:')\n",
    "\n",
    "nb_its_dev = log_interval\n",
    "flat_ims = True\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# dataset\n",
    "cprint('c', '\\nData:')\n",
    "\n",
    "# load data\n",
    "\n",
    "# data augmentation\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True,\n",
    "                                              num_workers=0)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True,\n",
    "                                            num_workers=0)\n",
    "\n",
    "else:\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=False,\n",
    "                                              num_workers=0)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n",
    "                                            num_workers=0)\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# net dims\n",
    "cprint('c', '\\nNetwork:')\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "\n",
    "net = BNN_cat(channels_in = 1, side_in = image_trans_size, classes = classes, N_train = NTrainPoints, lr = lr, cuda = use_cuda, grad_std_mul = 20)\n",
    "\n",
    "\n",
    "\n",
    "with open(models_dir + '/state_dicts.pkl', 'rb') as input:\n",
    "    net.weight_set_samples = pickle.load(input)\n",
    "\n",
    "net.set_mode_train(False)\n",
    "#print('net', len(net.weight_set_samples))\n",
    "test_cost = 0  # Note that these are per sample\n",
    "test_err = 0\n",
    "\n",
    "test_predictions = np.zeros((test_data_len, classes))\n",
    "\n",
    "\n",
    "\n",
    "net.set_mode_train(False)\n",
    "\n",
    "for j, (x, y) in enumerate(valloader):\n",
    "    cost, err, probs = net.sample_eval(x, y, Nsamples, grad=False)  # , logits=True\n",
    "    #print('probssssssssssssss', probs)\n",
    "    test_cost += cost\n",
    "    test_err += err.cpu().numpy()\n",
    "    test_predictions[nb_samples:nb_samples + len(x), :] = probs.numpy()\n",
    "    nb_samples += len(x)\n",
    "\n",
    "# test_cost /= nb_samples\n",
    "test_err /= nb_samples\n",
    "cprint('b', '    Loglike = %5.6f, err = %1.6f\\n' % (-test_cost, test_err))\n",
    "\n",
    "x_dev = []\n",
    "y_dev = []\n",
    "for x, y in valloader:\n",
    "    x_dev.append(x.cpu().numpy())\n",
    "    y_dev.append(y.cpu().numpy())\n",
    "\n",
    "x_dev = np.concatenate(x_dev)\n",
    "y_dev = np.concatenate(y_dev)\n",
    "print(x_dev.shape)\n",
    "print(y_dev.shape)\n",
    "\n",
    "im_ind = np.random.randint(0, y_dev.shape[0])\n",
    "# im_ind = 90\n",
    "print(\"image number:\", im_ind)\n",
    "\n",
    "x, y = x_dev[im_ind], y_dev[im_ind]\n",
    "x_rot = np.expand_dims(ndim.interpolation.rotate(x[0, :, :], 0, reshape=False, cval=-0.42421296), 0)\n",
    "\n",
    "print(\"real number:\", y)\n",
    "\n",
    "plt.imshow(ndim.interpolation.rotate(x_dev[im_ind,0,:,:], 0, reshape=False))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3eb51c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d669b701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582a7318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd4ba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ims = []\n",
    "\n",
    "ims.append(x_rot[:, :, :])\n",
    "\n",
    "ims = np.concatenate(ims)\n",
    "\n",
    "net.set_mode_train(False)\n",
    "\n",
    "y = np.ones(ims.shape[0]) * y\n",
    "ims = np.expand_dims(ims, axis=1)\n",
    "\n",
    "cost, err, probs = net.sample_eval(torch.from_numpy(ims), torch.from_numpy(y), Nsamples,\n",
    "                                   grad=False)  # , logits=True\n",
    "\n",
    "predictions = probs.numpy()\n",
    "\n",
    "print(\"predictions\", predictions)\n",
    "\n",
    "print(\"error\", err.cpu().numpy())\n",
    "\n",
    "# predictions.max(axis=1)[0]\n",
    "# selections = (predictions[:,i] == predictions.max(axis=1))\n",
    "print(\"predict\", predictions.argmax())\n",
    "\n",
    "print(im_ind)\n",
    "\n",
    "print(valset[im_ind][1])\n",
    "\n",
    "print(valset.class_to_idx)\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "prob = []\n",
    "for i in range(0, test_data_len):\n",
    "    x, y = x_dev[i], y_dev[i]\n",
    "    x_rot = np.expand_dims(ndim.interpolation.rotate(x[0, :, :], 0, reshape=False, cval=-0.42421296), 0)\n",
    "    # print(\"real number:\",y)\n",
    "    y_true.append(y)\n",
    "    # plt.imshow( ndim.interpolation.rotate(x_dev[im_ind,0,:,:], 0, reshape=False))\n",
    "    # plt.show()\n",
    "    ims = []\n",
    "    ims.append(x_rot[:, :, :])\n",
    "    ims = np.concatenate(ims)\n",
    "    net.set_mode_train(False)\n",
    "    y = np.ones(ims.shape[0]) * y\n",
    "    ims = np.expand_dims(ims, axis=1)\n",
    "    cost, err, probs = net.sample_eval(torch.from_numpy(ims), torch.from_numpy(y), Nsamples=Nsamples,\n",
    "                                       grad=False)  # , logits=True\n",
    "    predictions = probs.numpy()\n",
    "    prob.append(predictions)\n",
    "    #     print(\"predictions\", predictions)\n",
    "    #     print(\"error\", err.cpu().numpy())\n",
    "    y_pred.append(predictions.argmax())\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "prob = np.array(prob)\n",
    "prob = prob.reshape(test_data_len, classes)\n",
    "\n",
    "if save_data == True:\n",
    "    save_path = 'SGHMC_predict_data'\n",
    "    mkdir(save_path)\n",
    "    file_name = \"SGHMC_epochs=%d_lr=%f_batch_size=%d_image_trans_size=%d.csv\" \\\n",
    "                % (nb_epochs, lr, batch_size, image_trans_size)\n",
    "    completeName = os.path.join(save_path, file_name)\n",
    "    print('c', completeName)\n",
    "    if os.path.exists(completeName):\n",
    "        os.remove(completeName)\n",
    "    # df = pd.DataFrame(prob)\n",
    "    # df.to_csv(completeName)\n",
    "    np.savetxt(completeName, prob, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6119637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd711aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8e36f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ee1c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4866cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af971be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7fd483",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a8cf33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b15f50e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b809e8fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d753102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07df933c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546544ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a3b520",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85919aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9e01cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91533bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3312743f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e7616f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba54c0f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a4bdbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e075b91c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b685d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef6c1e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a74a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6839f597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5294d82c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7b61db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5adff57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eab0bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a543d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a98d53b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a41b358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350b42aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
